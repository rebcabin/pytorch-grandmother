# -*- coding: utf-8 -*-
"""Copy of 1 - Multilayer Perceptron.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tWl3K35pfLJy4PRHpGmNaprE2w-ZD7Wn

Mods by Brian Beckman, GSI Technology

# 1 - Multilayer Perceptron

In this series, we build machine-learning models
(specifically, neural networks) to perform image
classification using PyTorch and Torchvision.

In this first notebook, we start with one of the most basic
neural network architectures, a multilayer perceptron (MLP),
also known as a feedforward network. The dataset we'll be
using is the famous MNIST dataset, a dataset of 28x28 black
and white images consisting of handwritten digits, 0 to 9.

The following links to an image.

![](https://github.com/bentrevett/pytorch-image-classification/blob/master/assets/mlp-mnist.png?raw=1)

Process the dataset, build the model, and then train the
model. Afterwards, we'll do a short dive into what the model
has actually learned.

### Data Processing

Start by importing all the modules we'll need. The main ones
we need to import are:

- torch for general PyTorch functionality

- torch.nn and torch.nn.functional for neural-network-based
  functions

- torch.optim for the optimizer, which will update the
  parameters --- weights and biases --- of the neural network

- torch.utils.data for handling the dataset

- torchvision.transforms for data augmentation

- torchvision.datasets for loading the dataset

- sklearn's metrics for visualizing a confusion matrix

- sklearn's decomposition and manifold for visualizing the
  neural network's representations in two dimensions

- matplotlib for plotting

"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torch.utils.data as data

import torchvision.transforms as transforms
import torchvision.datasets as datasets

from sklearn import metrics
from sklearn import decomposition
from sklearn import manifold
from tqdm.notebook import trange, tqdm
import matplotlib.pyplot as plt
import numpy as np

import copy
import random
import time

"""To ensure reproducible results, set the random seed for
Python, Numpy and PyTorch."""

SEED = 1234

random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.cuda.manual_seed(SEED)
torch.backends.cudnn.deterministic = True

"""The first thing it to load the dataset.

This next code block automatically downloads the training set
for MNIST and saves it in a folder called `.data`. It creates
the folder if necessary.

"""

ROOT = '.data'

train_data = datasets.MNIST(root=ROOT,
                            train=True,
                            download=True)

"""Next, *normalize* the data. This means we want a mean of
zero and a standard deviation of one.

Why do this? Normalizing the data allows models to train
faster and to avoid local minima, i.e. to train more reliably.

Normalize the data by subtracting the mean and dividing by the
standard deviation. First, calculate the mean and standard
deviation. **Note**: it is important that the mean and
standard deviation are only calculated over the training set
and not the test set. Do not use any information at all from
the test set; only look at it when calculating the test loss.

To calculate the means and standard deviations, get the actual
data (the images) using the `.data.` attribute of the training
data, convert them into floating point, and then use the
built-in `mean` and `std` functions. The image data has values
between 0-255, which we scale between 0-1, so we divide by
255.

"""

mean = train_data.data.float().mean() / 255
std = train_data.data.float().std() / 255

print(f'Calculated mean: {mean}')
print(f'Calculated std: {std}')

"""Now we've calculated means and standard deviations, use
Torchvision's `transforms`.

A `transform` describes how to augment and process data. Data
augmentation artificially create more training examples. We
use `transforms.Compose` to built a sequence of
transformations to apply to images.

The transforms we use are:

- `RandomRotation` - randomly rotates the image between
  `(-x,+x)` degrees, where we have set `x = 5`. Note, the
  `fill=(0,)` is due to a
  [bug](https://github.com/pytorch/vision/issues/1759) in some
  versions of torchvision.

- `RandomCrop` - this first adds `padding` around an image, 2
  pixels here, to artificially make it bigger, before taking a
  random `28x28` square crop of the image.

- `ToTensor()` - this converts the image from a PIL image into
  a PyTorch tensor.

- `Normalize` - this subtracts the mean and divides by the
  standard deviations given.

We must apply the first two transformations before `ToTensor`
as they should both be applied on a PIL image. `Normalize`
should only be applied to the images after `ToTensor`. See the
Torchvision documentation for
[transforms that should be applied to PIL images](https://pytorch.org/vision/stable/transforms.html#transforms-on-pil-image-only)
and [transforms that should be applied on tensors](https://pytorch.org/vision/stable/transforms.html#transforms-on-torch-tensor-only).

We have two lists of transforms, one for training and one for
testing. The train transforms artificially create more
training examples. Do not augment test data in the same way.
We want a consistent set of examples on which to evaluate the
final model. The test data, however, should still be
normalized. """

train_transforms = transforms.Compose([
    transforms.RandomRotation(5, fill=(0,)),
    transforms.RandomCrop(28, padding=2),
    transforms.ToTensor(),
    transforms.Normalize(mean=[mean], std=[std])
])

test_transforms = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize(mean=[mean], std=[std])
])

"""load the train and test data with the relevant transforms.

"""

train_data = datasets.MNIST(root=ROOT,
                            train=True,
                            download=True,
                            transform=train_transforms)

test_data = datasets.MNIST(root=ROOT,
                           train=False,
                           download=True,
                           transform=test_transforms)

"""Check the `len` of the datasets to see how many examples
are within each."""

print(f'Number of training examples: {len(train_data)}')
print(f'Number of testing examples: {len(test_data)}')

"""Look at some images within the dataset to see what we're
working with. The function below plots a square grid of
images. If you supply less than a complete square number of
images it ignores the last few."""


def plot_images(images):
    n_images = len(images)

    rows = int(np.sqrt(n_images))
    cols = int(np.sqrt(n_images))

    fig = plt.figure()
    for i in range(rows * cols):
        ax = fig.add_subplot(rows, cols, i + 1)
        ax.imshow(images[i].view(28, 28).cpu().numpy(), cmap='bone')
        ax.axis('off')
    plt.show()


"""Load 25 images. These are processed through the training
transforms, so will be randomly rotated and cropped.

It's good practice to inspect data with transforms applied to
ensure they look sensible. For example, it wouldn't make sense
to flip the digits horizontally or vertically unless one
expects to see such in the test data. """

N_IMAGES = 25

images = [image
          for image, label in [train_data[i]
                               for i in range(N_IMAGES)]]

plot_images(images)

"""MNIST comes with a training and test set, but not a
validation set. We want a validation set to check how well the
model performs on unseen data. Why not use test data? We
should only measure performance over the test set once, after
all training is done. Think of the validation set as a proxy
test set we are allowed to look at as much as we want.

Create a validation set taking 10% of the training set.
**Note:** the validation set should always be created from the
training set. Never take the validation set from the test set.

Researchers should compare performance across the test set.
The only way to ensure this is a fair comparison is for all
researchers to use the same test set. If the validation set is
taken from the test set, then the test set is not the same as
everyone else's and the results cannot be compared.

First, define the exact number of examples that we want to be
in each split of the training/validation sets. """

VALID_RATIO = 0.9

n_train_examples = int(len(train_data) * VALID_RATIO)
n_valid_examples = len(train_data) - n_train_examples

"""Then, use `random_split` to take a random 10% of the
training set as a validation set. The remaining 90% stay in
the training set.

"""

train_data, valid_data = data.random_split(train_data,
                                           [n_train_examples, n_valid_examples])

"""Print out the number of examples again to check that the
splits are correct.

"""

print(f'Number of training examples: {len(train_data)}')
print(f'Number of validation examples: {len(valid_data)}')
print(f'Number of testing examples: {len(test_data)}')

"""The validation set has has the same transforms as the
 training set, with the random rotating and cropping. As we
 want the validation set to act as a proxy for the test set,
 it should also be fixed, without any random augmentation.

First, see what 25 of the images within the validation set
look like with the training transforms: """

N_IMAGES = 25

images = [image
          for image, label in [valid_data[i]
                               for i in range(N_IMAGES)]]

plot_images(images)

"""Replace the validation set's transform by overwriting it
with test transforms from above.

As the validation set is a `Subset` of the training set, if we
change the transforms of one, then, by default, Torchvision
changes the transforms of the other. To stop this, make a
`deepcopy` of the validation data. """

valid_data = copy.deepcopy(valid_data)
valid_data.dataset.transform = test_transforms

"""To double check we've correctly replaced the training
transforms, view the same set of images and notice how they're
more central (no random cropping) and have a more standard
orientation (no random rotations).

"""

N_IMAGES = 25

images = [image
          for image, label in [valid_data[i]
                               for i in range(N_IMAGES)]]

plot_images(images)

"""Next,'ll define a `DataLoader` for each of the
training/validation/test sets. Iterate over these, yielding
batches of images and labels for training.

We only need to shuffle the training set as it's used for
stochastic gradient descent. We want each batch to be
different between epochs. As we aren't using the validation or
test sets to update model parameters, they are not shuffled.

Ideally, use the biggest batch size we can. The 64 here is
relatively small and can be increased if the hardware can
handle it. """

BATCH_SIZE = 64

train_iterator = data.DataLoader(train_data,
                                 shuffle=True,
                                 batch_size=BATCH_SIZE)

valid_iterator = data.DataLoader(valid_data,
                                 batch_size=BATCH_SIZE)

test_iterator = data.DataLoader(test_data,
                                batch_size=BATCH_SIZE)

"""### Defining the Model

The model is a multilayer perceptron (MLP) with two hidden
layers. The image below shows the archicture of the model.

![](https://github.com/bentrevett/pytorch-image-classification/blob/master/assets/mlp-mnist.png?raw=1)

First, flatten each 1x28x28 (1 color channel, 28 pixels height
and width) image into a 784-element vector. The 784 elements
are called *features*. We flatten the input because MLPs
cannot handle two or three-dimensional data. In different
senses of "dimension," the features form a one-dimensional
array representing a 784-dimensional feature vector.

The 784-dimensional input is passed through the first hidden
layer to transform it into 250 dimensions. Then through
another hidden layer, transforming it to 100 dimensions.
Finally, through an output layer that transform data into a
10-dimensional vector. The output dimension should equal the
number of classes in the output. Here we have ten numerals,
0-9, so need the output to be 10 dimensions.

The transformation layers between 784 to 250, 250 to 100 and
100 to 10 dimensions are `Linear` layers. These are also known
as "fully connected" layers because every element in one layer
is connected to every element in the next. We can think of
these elements as *neurons*, as this architecture is inspired
by how the human brain is made of millions of interconnected
nodes, also called neurons.

Each connection between a neuron in one layer and a neuron in
the next has a *weight* associated with it. The input to one
neuron is the sum of the weighted values of all neurons in the
previous layer connected to it, plus a weighted bias term,
where the bias value is always 1. Due to the presence of bias
terms, the layers are also known as "affine."

Each neuron applies an *activation function* to its input
weighted sum. This activation function is a non-linear
function that allows the neural network to learn non-linear
functions between inputs and outputs.

We define the MLP below with three linear layers. We first
take the input batch of images and flatten them, so they can
be passed into the linear layers. We then pass them through
the first linear layer, `input_fc`, which calculates the
weighted sum of the inputs, and then apply the *ReLU*
(rectified linear unit) activation function elementwise. This
result is then passed through another linear layer,
`hidden_fc`, again applying the same activation function
elementwise. Finally, we pass this through the final linear
layer, `output_fc`. We return not only the output but also the
second hidden layer as we will do some analysis on it later.

The ReLU activation function is a popular non-linear function
that is simply $max(0, x)$, where $x$ is the weighted sum of
the inputs to that neuron. Other popular activation functions
are hyperbolic tan (tanh) and sigmoid function, however ReLU
is most commonly used nowadays.

![](https://github.com/bentrevett/pytorch-image-classification/blob/master/assets/relu.png?raw=1)

Note that we do not use an activation function on the input
directly nor or on the output. Never use activation functions
directly on the input. For numerical stability, PyTorch
combines activation functions for the output with the
functions that calculate the *loss*, also known as *error* or
*cost*, of a neural network.

There is no magic formula for how many layers to use and how
many neurons to have in each layer, and there is likely a
better set of values. However, the general idea is that neural
networks extract features from data. Layers closer to the
input learn to extract general features (e.g. lines, curves,
edges), whilst later layers combine the features extracted
from previous layers into more high-level features (e.g. the
intersection of two lines making a cross, multiple curves make
a circle). We force the neural network to learn these features
by reducing the number of neurons in each layer. This way, it
must learn to compress information by extracting only the
useful and general features. Thus, we want a neural network
with multiple layers and some sort of information compression
(reduced number of neurons in subsequent layers).

"""


class MLP(nn.Module):
    def __init__(self, input_dim, output_dim):
        super().__init__()

        self.input_fc = nn.Linear(input_dim, 250)
        self.hidden_fc = nn.Linear(250, 100)
        self.output_fc = nn.Linear(100, output_dim)

    def forward(self, x):
        # x = [batch size, height, width]
        batch_size = x.shape[0]
        x = x.view(batch_size, -1)
        # x = [batch size, height * width]
        h_1 = F.relu(self.input_fc(x))
        # h_1 = [batch size, 250]
        h_2 = F.relu(self.hidden_fc(h_1))
        # h_2 = [batch size, 100]
        y_pred = self.output_fc(h_2)
        # y_pred = [batch size, output dim]
        return y_pred, h_2


"""Define the model by creating an instance of MLP and setting
the correct input and output dimensions."""

INPUT_DIM = 28 * 28
OUTPUT_DIM = 10

model = MLP(INPUT_DIM, OUTPUT_DIM)

"""Create a small function to calculate the number of
trainable parameters (weights and biases) in the model.

"""


def count_parameters(model):
    return sum(p.numel()
               for p in model.parameters()
               if p.requires_grad)


"""The first layer has 784 neurons connected to 250 neurons,
thus 784*250 weighted connections plus 250 bias terms.

The second layer has 250 neurons connected to 100 neurons,
thus 250*100 weighted connections plus 100 bias terms.

The third layer has 100 neurons connected to 10 neurons,
100*10 weighted connections plus 10 bias terms.

$$784 \cdot 250 + 250 + 250 \cdot 100 + 100 + 100 \cdot 10 + 10 = 222,360 $$
"""

print(f'The model has {count_parameters(model):,} trainable parameters')

"""### Training the Model

Next, define the optimizer. This is the algorithm for updating
parameters (weights and biases) of the model with respect to
the loss calculated on the data.

We aren't going into too much detail on how neural networks
are trained (see
[this](http://neuralnetworksanddeeplearning.com/) article if
you want to know how) but the gist is:

- pass a batch of data through the model

- calculate the loss of a batch by comparing the model's
  predictions against actual labels (this is _supervised_
  learning)

- calculate the gradient of each parameter with respect to the
  loss

- update each parameter by subtracting it gradient multiplied
  by a small *learning rate* parameter

We choose the *Adam* algorithm with the model parameters. We
might improve results by searching over various optimizers and
learning rates. Default Adam is usually a good starting-off
point.

Check out
[this](https://ruder.io/optimizing-gradient-descent/) article
to learn more about various optimization algorithms. """

optimizer = optim.Adam(model.parameters())

"""Then, define a *criterion*, PyTorch's name for a
loss/cost/error function. This function will take in model
predictions and the actual labels, then compute the
loss/cost/error of the model with its current parameters.

`CrossEntropyLoss` both computes the *softmax* activation
function on the supplied predictions as well as the actual
loss via *negative log likelihood*.

Briefly, softmax is:

$$\text{softmax }(\mathbf{x}) = \frac{e^{x_i}}{\sum_j e^{x_j}}$$

This turns out 10-dimensional output, where each element is an
unbounded real number, into a probability distribution over 10
elements. That is, all values are between 0 and 1, and
together they all sum to 1.

Why turn things into a probability distribution? So we have
_negative log likelihood_ for the loss function, which expects
probabilities. PyTorch calculates negative log likelihood for
a single example via:

$$\text{negative log likelihood }(\mathbf{\hat{y}}, y) = -\log \big( \text{softmax}(\mathbf{\hat{y}})[y] \big)$$

$\mathbf{\hat{y}}$ is the $\mathbb{R}^{10}$ output, from the
neural network, whereas $y$ is the supervisory (user-supplied)
label, an integer representing the class. The loss is the
negative log of the class index of the softmax. For example:

$$\mathbf{\hat{y}} = [5,1,1,1,1,1,1,1,1,1]$$

$$\text{softmax }(\mathbf{\hat{y}}) = [0.8585, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157]$$

If the label was class zero, the loss would be:

$$\text{negative log likelihood }(\mathbf{\hat{y}}, 0) = - \log(0.8585) = 0.153 \dots$$

If the label was class five, the loss would be:

$$\text{negative log likelihood }(\mathbf{\hat{y}}, 5) = - \log(0.0157) = 4.154 \dots$$

Intuitively, as the model's output corresponding to the
correct class index increases, loss decreases.

"""

criterion = nn.CrossEntropyLoss()

"""Then, define `device`. This places the model and data into
a GPU, if you have one, otherwise into the CPU.

"""

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

"""Place the model and criterion into the device by using the
`.to` method.

"""

model = model.to(device)
criterion = criterion.to(device)

"""Next, define a function to calculate the accuracy of the
model. This takes the index of the highest value for a
prediction and compares it against the actual class label. We
then divide the number that the model got correct by the
number of samples in the batch to calculate accuracy across
the batch.

"""


def calculate_accuracy(y_pred, y):
    top_pred = y_pred.argmax(1, keepdim=True)
    correct = top_pred.eq(y.view_as(top_pred)).sum()
    acc = correct.float() / y.shape[0]
    return acc


"""Finally, define the training loop:

- put the model into `train` mode

- iterate over the training dataloader, returning batches of
  (image, label)

- place the batch on to the device

- clear gradients calculated from the last batch

- pass our batch of images, `x`, through the model to get
  predictions, `y_pred`

- calculate the loss between predictions and the actual,
  supervisory labels

- calculate the accuracy between predictions and the actual
  labels

- calculate the gradients of each parameter

- update parameters with an optimizer step

- update metrics

Some layers act differently when training and evaluating the
model. Therefore we must tell the model we are in "training"
mode. """


def train(model, iterator, optimizer, criterion, device):
    epoch_loss = 0
    epoch_acc = 0

    model.train()

    for (x, y) in tqdm(iterator, desc="Training", leave=False):
        x = x.to(device)
        y = y.to(device)

        optimizer.zero_grad()

        y_pred, _ = model(x)

        loss = criterion(y_pred, y)

        acc = calculate_accuracy(y_pred, y)

        loss.backward()

        optimizer.step()

        epoch_loss += loss.item()
        epoch_acc += acc.item()

    return epoch_loss / len(iterator), epoch_acc / len(iterator)


"""The evaluation loop is similar to the training loop. The
differences are:

- put the model into evaluation mode with `model.eval()`

- wrap iterations inside a `with torch.no_grad()`

- do not zero gradients as we are not calculating any

- do not calculate gradients as we are not updating parameters

- do not take an optimizer step as we are not calculating
  gradients

`torch.no_grad()` ensures that gradients are not calculated
for greater speed and less memory. """


def evaluate(model, iterator, criterion, device):
    epoch_loss = 0
    epoch_acc = 0

    model.eval()

    with torch.no_grad():
        for (x, y) in tqdm(iterator, desc="Evaluating", leave=False):
            x = x.to(device)
            y = y.to(device)

            y_pred, _ = model(x)

            loss = criterion(y_pred, y)

            acc = calculate_accuracy(y_pred, y)

            epoch_loss += loss.item()
            epoch_acc += acc.item()

    return epoch_loss / len(iterator), epoch_acc / len(iterator)


"""The final step before training to define a small
function to tell us how long an epoch took."""


def epoch_time(start_time, end_time):
    elapsed_time = end_time - start_time
    elapsed_mins = int(elapsed_time / 60)
    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))
    return elapsed_mins, elapsed_secs


"""Train!

During each epoch, calculate training loss and accuracy,
followed by validation loss and accuracy. Check whether the
validation loss achieved is the best so far. If so, we save
the model's parameters (called a `state_dict`). """

EPOCHS = 10

best_valid_loss = float('inf')

for epoch in trange(EPOCHS):  # trange is a timed range

    start_time = time.monotonic()

    train_loss, train_acc = train(model, train_iterator, optimizer, criterion, device)
    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion, device)

    if valid_loss < best_valid_loss:
        best_valid_loss = valid_loss
        torch.save(model.state_dict(), 'tut1-model.pt')

    end_time = time.monotonic()

    epoch_mins, epoch_secs = epoch_time(start_time, end_time)

    print(f'Epoch: {epoch + 1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')
    print(f'\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc * 100:.2f}%')
    print(f'\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc * 100:.2f}%')

"""Afterwards, load the parameters of the model that achieved
the best validation loss and then use this to evaluate on the
test set."""

model.load_state_dict(torch.load('tut1-model.pt'))

test_loss, test_acc = evaluate(model, test_iterator, criterion, device)

"""Our model achieves at least 98% accuracy on the test set.

improve the accuracy by tweaking hyperparameters, e.g. number
of layers, number of neurons per layer, optimization algorithm
used, learning rate, batch size, number of epochs, etc.

"""

print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc * 100:.2f}%')

"""### Examining the Model

Now that we have a trained model, there are a few things we
can look at. Most of these are simple exploratory analysis,
but they can offer some insights into a model.

Check what examples your model gets wrong and ensure that
they're reasonable mistakes.

The function below returns the model's predictions over a
given dataset. It returns the inputs (image) the outputs
(model predictions) and the ground-truth, supervisory labels.
"""


def get_predictions(model, iterator, device):
    model.eval()

    images = []
    labels = []
    probs = []

    with torch.no_grad():
        for (x, y) in iterator:
            x = x.to(device)

            y_pred, _ = model(x)

            y_prob = F.softmax(y_pred, dim=-1)

            images.append(x.cpu())
            labels.append(y.cpu())
            probs.append(y_prob.cpu())

    images = torch.cat(images, dim=0)
    labels = torch.cat(labels, dim=0)
    probs = torch.cat(probs, dim=0)

    return images, labels, probs


"""Get these predictions and, by taking the index of the
highest predicted probability, get the predicted labels."""

images, labels, probs = get_predictions(model, test_iterator, device)

pred_labels = torch.argmax(probs, 1)

"""Make a confusion matrix from our actual labels and our predicted labels."""


def plot_confusion_matrix(labels, pred_labels):
    fig = plt.figure(figsize=(10, 10))
    ax = fig.add_subplot(1, 1, 1)
    cm = metrics.confusion_matrix(labels, pred_labels)
    cm = metrics.ConfusionMatrixDisplay(cm, display_labels=range(10))
    cm.plot(values_format='d', cmap='Blues', ax=ax)
    plt.show()


"""The results seem reasonable enough. The most confused
predictions-actuals are: 3-5 and 2-7."""

plot_confusion_matrix(labels, pred_labels)

"""Next, for each example, check whether the predicted label
matches the actual label."""

corrects = torch.eq(labels, pred_labels)

"""loop through all example predictions and store all
incorrect examples into an array. Sort these incorrect
examples by how confident they were, most confident first.

"""

incorrect_examples = []

for image, label, prob, correct in zip(images, labels, probs, corrects):
    if not correct:
        incorrect_examples.append((image, label, prob))

incorrect_examples.sort(reverse=True,
                        key=lambda x: torch.max(x[2], dim=0).values)

"""Plot the incorrectly predicted images along with the actual
label and confidence of the incorrect label."""


def plot_most_incorrect(incorrect, n_images):
    rows = int(np.sqrt(n_images))
    cols = int(np.sqrt(n_images))

    fig = plt.figure(figsize=(20, 10))
    for i in range(rows * cols):
        ax = fig.add_subplot(rows, cols, i + 1)
        image, true_label, probs = incorrect[i]
        true_prob = probs[true_label]
        incorrect_prob, incorrect_label = torch.max(probs, dim=0)
        ax.imshow(image.view(28, 28).cpu().numpy(), cmap='bone')
        ax.set_title(f'true label: {true_label} ({true_prob:.3f})\n'
                     f'pred label: {incorrect_label} ({incorrect_prob:.3f})')
        ax.axis('off')
    fig.subplots_adjust(hspace=0.5)
    plt.show()


"""Below, we can see the 25 images the model got incorrect and
was most confident about.

A lot of these digits are irregular, so it's difficult for the
model to do well on these. For the images that look fine, if
you squint you can sort of see why the model got it wrong.

Why is the neural network so confident on the irregular
digits? Surely, if it's a weird-looking digit, then the output
of softmax should be nearly evenly distributed across a few
digits the model isn't sure about, right? Well, no. The model
has been trained to only be incredibly confident about its
predictions. Thus, when it sees an image, it will always be
confident about what it is. """

N_IMAGES = 25

plot_most_incorrect(incorrect_examples, N_IMAGES)

"""Visualize the output and intermediate representations from the
model.

The function below loops through the provided dataset, gets
the output from the model and the intermediate representation
from the layer before that, the second hidden layer. """


def get_representations(model, iterator, device):
    model.eval()

    outputs = []
    intermediates = []
    labels = []

    with torch.no_grad():
        for (x, y) in tqdm(iterator):
            x = x.to(device)

            y_pred, h = model(x)

            outputs.append(y_pred.cpu())
            intermediates.append(h.cpu())
            labels.append(y)

    outputs = torch.cat(outputs, dim=0)
    intermediates = torch.cat(intermediates, dim=0)
    labels = torch.cat(labels, dim=0)

    return outputs, intermediates, labels


"""Run the function to get the representations."""

outputs, intermediates, labels = get_representations(model,
                                                     train_iterator,
                                                     device)

"""The data to visualize is in ten dimensions and 100
dimensions. Reduce the data two dimensions to plot it.

The first technique is PCA (principal component analysis).
Define a function to calculate the PCA of our data, and then
define a function to plot it.

"""


def get_pca(data, n_components=2):
    pca = decomposition.PCA()
    pca.n_components = n_components
    pca_data = pca.fit_transform(data)
    return pca_data


def plot_representations(data, labels, n_images=None):
    if n_images is not None:
        data = data[:n_images]
        labels = labels[:n_images]
    fig = plt.figure(figsize=(10, 10))
    ax = fig.add_subplot(111)
    scatter = ax.scatter(data[:, 0], data[:, 1], c=labels, cmap='tab10')
    handles, labels = scatter.legend_elements()
    ax.legend(handles=handles, labels=labels)
    plt.show()


"""First, plot the representations from the ten-dimensional
output layer, reduced to two dimensions."""

output_pca_data = get_pca(outputs)
plot_representations(output_pca_data, labels)

"""Next, plot the outputs of the second hidden layer.

The clusters seem similar to the one above. In fact, if we
rotated the below image anti-clockwise, it wouldn't be too far
off the PCA of the output representations. """

intermediate_pca_data = get_pca(intermediates)
plot_representations(intermediate_pca_data, labels)

"""An alternative to PCA is t-SNE (t-distributed stochastic
neighbor embedding).

This is commonly thought of as "better" than PCA, although it
can be
[misinterpreted](https://distill.pub/2016/misread-tsne/). """


def get_tsne(data, n_components=2, n_images=None):
    if n_images is not None:
        data = data[:n_images]
    tsne = manifold.TSNE(n_components=n_components, random_state=0)
    tsne_data = tsne.fit_transform(data)
    return tsne_data


"""t-SNE is very slow, so we only compute it on a subset of
the representations.

The classes look well separated, and it is possible to use
[k-NN](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm)
on this representation for better decent accuracy. """

N_IMAGES = 5_000

output_tsne_data = get_tsne(outputs, n_images=N_IMAGES)
plot_representations(output_tsne_data, labels, n_images=N_IMAGES)

"""Plot the intermediate representations on the same subset.

Again, the classes look well separated, though less so than
the output representations. This is because these
representations are intermediate features for weighing up
evidence of the digit in the image. Hence, in theory, the
classes should become more separated the closer we are to the
output layer, exactly what we see here. """

intermediate_tsne_data = get_tsne(intermediates, n_images=N_IMAGES)
plot_representations(intermediate_tsne_data, labels, n_images=N_IMAGES)

"""Another experiment is to generate fake digits.

The function below repeatedly generates random noise and feeds
it through the model to find the most confidently generated
digit for the desired class. """


def imagine_digit(model_, digit, device_, n_iterations=50_000):
    model_.eval()

    best_prob_ = 0
    best_image_ = None

    with torch.no_grad():

        for iteration in trange(n_iterations):

            x = torch.randn(32, 28, 28).to(device_)

            y_pred, _ = model_(x)

            preds = F.softmax(y_pred, dim=-1)

            _best_prob, index = torch.max(preds[:, digit], dim=0)

            if _best_prob > best_prob_:
                best_prob_ = _best_prob
                best_image_ = x[index]

            if ((iteration + 1) % 2500) == 0:
                print(f'{iteration = }, {best_prob_ = }, {index = }')

    return best_image_, best_prob_


"""Let's try to generate a perfect three."""

DIGIT = 3

print("imagining the numeral 3 --- takes a long time...")
best_image, best_prob = imagine_digit(model, DIGIT, device)

"""Looking at the best probability achieved, we have a digit
that the model is 100% confident is a three."""

print(f'Best image probability: {best_prob.item() * 100:.2f}%')

"""Unfortunately, the imagined perfect three just looks like
random noise.

As mentioned before, the model has only been trained to be
incredibly confident with its predictions. When faced with
random noise, it will classify it as something.

It is also possible that the model is *overfitting* on the
training set - that there is a common pattern in handwritten
threes within the training set, but it's not the pattern we
want our model to learn. """

plt.imshow(best_image.cpu().numpy(), cmap='bone')
plt.axis('off')
plt.show()

"""Finally, plot the weights in the first layer of our model.

The hope is that there's one neuron in this first layer that's
learned to look for certain patterns in the input and thus has
high weight indicating this pattern. If we plot these weights,
we should see these patterns. """


def plot_weights(weights_, n_weights):
    rows = int(np.sqrt(n_weights))
    cols = int(np.sqrt(n_weights))

    fig = plt.figure(figsize=(20, 10))
    for i in range(rows * cols):
        ax = fig.add_subplot(rows, cols, i + 1)
        ax.imshow(weights_[i].view(28, 28).cpu().numpy(), cmap='bone')
        ax.axis('off')
    plt.show()


"""A few weights look like random noise but some of them do
have weird patterns. These patterns show "ghostly"
image-looking shapes, but are clearly not images."""

N_WEIGHTS = 25

weights = model.input_fc.weight.data

print("Plotting weights ...")
plot_weights(weights, N_WEIGHTS)

"""### Conclusions

In this notebook we have shown:

- loading Torchvision datasets

- loading transforms to augment and normalize data

- defining a MLP

- training a model to achieve >98% accuracy

- viewing mistakes

- visualizing data in lower dimensions with PCA and t-SNE

- generating fake digits

- viewing the learned weights of our model

In the next notebook, we'll implement a convolutional neural
network (CNN) and evaluate it on the MNIST dataset. """
