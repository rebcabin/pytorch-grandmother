{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing the Simplest Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T01:33:04.850329Z",
     "start_time": "2023-11-02T01:33:04.546497Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T01:33:04.995115Z",
     "start_time": "2023-11-02T01:33:04.855379Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy\n",
    "import pandas\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This walk-through of creating a simple neural network to predict outputs given an input is derived from [Neural Networks from Scratch with Python Code and Math in Detail](https://medium.com/towards-artificial-intelligence/building-neural-networks-from-scratch-with-python-code-and-math-in-detail-i-536fae5d7bbf).  I found that walk through had a few errors and some confusing/imprecise language, so I made this notebook to walk me through each step.  This exercise makes most sense if you give that page a skim first, since I assume that you already have a basic understanding of what a neural network _does_.  Here, we describe mathematically how a neural network actually works.\n",
    "\n",
    "We implement a neural network using only numpy, then go on to implement the same network using PyTorch to illustrate what PyTorch is doing underneath its convenient interface.  Via the same random number seeds, our hand-rolled version is mathematically identical to the much more terse PyTorch version.\n",
    "\n",
    "Finally, if you're viewing this in webpage form, you can find the [actual Jupyter Notebook in GitHub](https://github.com/glennklockwood/limelead/blob/master/notebooks/perceptron.ipynb).\n",
    "\n",
    "## Problem statement\n",
    "\n",
    "Here, we build a small linear model (a [perceptron](https://en.wikipedia.org/wiki/Perceptron)) that classifies an input as one thing or another (_binary classification_) via a couple of input parameters.\n",
    "\n",
    "Specifically, we create a model of a simple OR gate with two inputs and one output.  OR gates have the following behavior:\n",
    "\n",
    "1. If both inputs are 0 (off), the output is 0 (off)\n",
    "2. If both inputs are 1 (on), the output is 1 (on)\n",
    "3. If either input is 1 (on), the output is still 1 (on)\n",
    "\n",
    "We want a model, $f(x)$, that takes two inputs ($x$) and returns an output of either 0 or 1 based on the value of those inputs.\n",
    "\n",
    "Let's first build a set of observations that capture this behavior as if we were experimenting with an OR gate ourselves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T01:37:03.960781Z",
     "start_time": "2023-11-02T01:37:03.924911Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs and true outputs (truth table) are:\n",
      "               input 1  input 2  true output\n",
      "observation #                               \n",
      "0                    0        0            0\n",
      "1                    0        1            1\n",
      "2                    1        0            1\n",
      "3                    1        1            1\n"
     ]
    }
   ],
   "source": [
    "inputs = pandas.DataFrame(\n",
    "    [[0, 0],\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "    [1, 1]],\n",
    "    columns=[\"input 1\", \"input 2\"])\n",
    "inputs.index.name = \"observation #\"\n",
    "\n",
    "ground_truth = pandas.Series([0, 1, 1, 1], name=\"true output\", index=inputs.index)\n",
    "\n",
    "print(\"Inputs and true outputs (truth table) are:\")\n",
    "print(pandas.concat((inputs, ground_truth), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining our model\n",
    "\n",
    "### Linear model\n",
    "\n",
    "The model we're using to predict our output values, $f(x, w)$, is based on a linear model $y(x)$ at its core:\n",
    "\n",
    "$ y(x) = x \\cdot w + b $\n",
    "\n",
    "where\n",
    "\n",
    "- $x$ are our observed inputs (the _independent_ variables)\n",
    "- $w$ are empirical \"weight\" parameters\n",
    "- $b$ is an empirical \"bias\" parameter\n",
    "\n",
    "The magic of this model is in figuring out what values for our empirical parameters, $w$ and $b$, result in the model producing our true outputs for each set of given inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(x, weights, bias):\n",
    "    \"\"\"Linear model function\n",
    "    \"\"\"\n",
    "    return numpy.dot(x, weights) + bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our specific case, we have two inputs, so the actual model will be\n",
    "\n",
    "$ y(x_1, x_2) = x_1 w_1 + x_2 w_2 + b $\n",
    "\n",
    "or more generally,\n",
    "\n",
    "$ y(\\mathbf{x}) = \\mathbf{x} \\cdot \\mathbf{w} + b $\n",
    "\n",
    "The lattermost form is the most rigorous, but I may just refer to _vectors_ like $\\mathbf{x}$ and $\\mathbf{w}$ as $x$ and $w$ hereafter to match how they are represented in Python.\n",
    "\n",
    "### Activation function\n",
    "\n",
    "We also have to shove this $y(x)$ through an \"activation function\" $A(x)$ to make so the model can only return values between 0 (off) and 1 (on) though.  We arbitrarily choose a sigmoid function to accomplish this:\n",
    "\n",
    "$ \\displaystyle A(y) = \\frac{1}{1 + e^{-y}} $\n",
    "\n",
    "Which can be defined in Python as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5a8f37b8>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"Activation function\n",
    "    \"\"\"\n",
    "    return 1.0 / (1.0 + numpy.exp(-x))\n",
    "\n",
    "matplotlib.pyplot.plot(numpy.arange(-10, 10, 0.1), sigmoid(numpy.arange(-10, 10, 0.1)), label=\"sigmoid\")\n",
    "matplotlib.pyplot.grid()\n",
    "matplotlib.pyplot.title(\"Sigmoid activation function\")\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we're nesting our linear function in an activation function, $f(x, w)$ looks pretty gnarly if you expand it out:\n",
    "\n",
    "$ f(x) = A(y(x)) = A(x \\cdot w + b) = \\frac{1}{1 + e^{-{x} \\cdot w - b}} $\n",
    "\n",
    "But I like to really think of our model as $ y(x) = x \\cdot w + b $ with some special sauce splashed on top to make sure our output values aren't unbounded.  If we didn't have an activation function, our linear model $y(x)$ would predict values that could go to infinity which\n",
    "\n",
    "1. doesn't represent the reality of the true outputs (we know that they can only vary from 0 to 1)\n",
    "2. will cause our model to not _converge_ (approach the right answer) very well\n",
    "\n",
    "You can confirm this yourself by defining `sigmoid(x)` to just `return x`.  There's a much more rigorous explanation of what the activation function plays in neural networks, so please look it up if you're interested.\n",
    "\n",
    "Now that we have a model in mind, we have to figure out how to _parameterize_ to get values for our weights "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up gradient descent\n",
    "\n",
    "We have a choice of model (a linear one wrapped in an activation function), so now we need to figure out what values of our empirical parameters $w$ and $b$ should be to make the model produce outputs that most closely track reality.  To do this, we use [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent).  Its formal definition is\n",
    "\n",
    "$ w_{t+1} = w_{t} - R \\cdot \\frac{d}{dw}E(x, w_{t}) $\n",
    "\n",
    "where $R$ is the \"learning rate,\" an empirical _hyperparameter_.  Whereas $w$ and $b$ are parameters that relate inputs to outputs, $R$ is a hyperparameter because it doesn't relate to what's being modeled as much as it relates to the model itself.\n",
    "\n",
    "$E(x, w)$ is a \"loss function\" that describes how far away our empirical model $f(x, w)$ is from ground truth values $ f_{0} $, and the goal of this whole exercise with gradient descent is to minimize the loss, or make the difference between our model and reality as small as possible.\n",
    "\n",
    "Our first order of business to actually calculate some numbers and evolve our model is to define some starting values of our empirical parameters: weights, bias, and learning rate.  Since our inputs $x$ are actually an array, $w$ must also be an array with one value for each of our input variables in $x$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting initial weights to random values.\n",
      "           weight\n",
      "input 1  0.417022\n",
      "input 2  0.720324\n",
      "Setting starting bias to a random value: 0.000114\n"
     ]
    }
   ],
   "source": [
    "numpy.random.seed(seed=1)\n",
    "print(\"Setting initial weights to random values.\")\n",
    "weights = numpy.random.rand(inputs.shape[1], 1)\n",
    "\n",
    "print(pandas.DataFrame(weights, index=inputs.columns, columns=[\"weight\"]))\n",
    "\n",
    "bias = numpy.random.rand(1)[0]\n",
    "print(\"Setting starting bias to a random value: {:4f}\".format(bias))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our bias parameter $b$ and learning rate (hyper)parameter $R$ are both scalar values.\n",
    "\n",
    "We can set the bias to some random value because gradient descent will find its optimal value for us.\n",
    "\n",
    "Learning rate is a constant that we set--it needs to be small enough for the interative method to not go haywire by skipping around too much between successive iterations, but not so small as to take a gazillion iterations to approach the optimal weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting learning rate to 0.05 based on prior experience.\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.05\n",
    "print(\"Setting learning rate to {} based on prior experience.\".format(learning_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying gradient descent\n",
    "\n",
    "How do we actually _solve_ our gradient descent equation, $ w_{t+1} = w_{t} - R \\cdot \\frac{d}{dw}E(x, w_{t}) $?  That is, how do we train our model?  Well, we know the following:\n",
    "\n",
    "- $w_{t}$:  we randomly defined them above\n",
    "- $ y(\\mathbf{x}) = \\mathbf{x} \\cdot \\mathbf{w} + b  $\n",
    "- $ f(y) = \\frac{1}{1+e^{-y}}$\n",
    "- _learning rate_ $R = 0.05$\n",
    "\n",
    "So we just need to figure out what $\\frac{dE}{dw}$ is--or how our _loss function_ varies as our _weights_ vary for a given set of input values $x$.\n",
    "\n",
    "We can pretty arbitrarily define our _loss function_.  This form seems to capture the right idea:\n",
    "\n",
    "$ E = abs \\left ( f_{0} - f \\right ) $\n",
    "\n",
    "As the difference between our predicted $f$ and ground truth outputs $f_{0}$ get bigger, our error gets bigger.  Easy.\n",
    "\n",
    "Then we break apart $\\frac{\\partial E}{\\partial w}$ using the chain rule:\n",
    "\n",
    "$ \\displaystyle \\frac{\\partial E}{\\partial w} =\n",
    "        \\frac{\\partial E}{\\partial f}\n",
    "  \\cdot \\frac{\\partial f}{\\partial y}\n",
    "  \\cdot \\frac{\\partial y}{\\partial w} $\n",
    "\n",
    "The derivations of these three components are pretty straightforward based on our definitions of $y(\\mathbf{x})$, $f(y)$, and $E$ above--apply your college calculus to derive them yourself, or just use Wolfram Alpha!  They come out to be:\n",
    "\n",
    "1. $ \\frac{\\partial E}{\\partial f} = \\frac{abs \\left ( f - f_{0} \\right )}{f - f_{0}} $\n",
    "2. $ \\frac{\\partial f}{\\partial y} = f \\left ( 1 - f \\right ) $\n",
    "3. $ \\frac{\\partial y}{\\partial w} = x_{0} $\n",
    "\n",
    "With this, we can just calculate $w_{t}$ for $t \\rightarrow \\infty$ using the gradient descent formula above to approach the optimal set of weights that minimize our loss function and thereby produce a model that approximates reality.  So as not to spend infinite time calculating for $t \\rightarrow \\infty$, we choose $t \\rightarrow 10000$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ITERATIONS = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then start the gradient descent process to slowly nudge $w$ and $b$ towards optimal values!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error at step     0:   1.47e+00\n",
      "error at step  1000:   2.33e-01\n",
      "error at step  2000:   1.09e-01\n",
      "error at step  3000:   6.98e-02\n",
      "error at step  4000:   5.10e-02\n",
      "error at step  5000:   4.00e-02\n",
      "error at step  6000:   3.29e-02\n",
      "error at step  7000:   2.79e-02\n",
      "error at step  8000:   2.43e-02\n",
      "error at step  9000:   2.14e-02\n",
      "Final weights: [9.98578506 9.98589032]\n",
      "Final bias:    -4.52752860008788\n",
      "10000 iterations took 1.9 seconds\n"
     ]
    }
   ],
   "source": [
    "# convert dataframe bits into arrays for processing\n",
    "inputs_array = inputs.to_numpy()\n",
    "truth_array = ground_truth.to_numpy().reshape(-1, 1)\n",
    "\n",
    "x = inputs_array\n",
    "t0 = time.time()\n",
    "for i in range(NUM_ITERATIONS):\n",
    "    y = linear(x, weights, bias)\n",
    "    f = sigmoid(y)\n",
    "    \n",
    "    error = numpy.abs(f - truth_array)\n",
    "    \n",
    "    # calculate out partial derivatives for each input\n",
    "    dE_df = error/(f - truth_array)\n",
    "    df_dy = sigmoid(y) * (1.0 - sigmoid(y))\n",
    "    dy_dw = x\n",
    "    dE_dy = dE_df * df_dy\n",
    "    dE_dw = numpy.dot(dy_dw.T, dE_dy)  # dy_dw = x\n",
    "\n",
    "    # update weights and biases - the error is the sum of error over each input\n",
    "    weights -= learning_rate * dE_dw\n",
    "    bias -= learning_rate * dE_dy.sum()\n",
    "\n",
    "    if i % (NUM_ITERATIONS / 10) == 0:\n",
    "        print(\"error at step {:5d}: {:10.2e}\".format(i, error.sum()))\n",
    "\n",
    "print(\"Final weights: {}\".format(weights.flatten()))\n",
    "print(\"Final bias:    {}\".format(bias))\n",
    "print(\"{:d} iterations took {:.1f} seconds\".format(NUM_ITERATIONS, time.time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have an optimal set of weights $\\mathbf{w}$.  To make predictions using these weights, we just run our inputs through $ f(x) = A(y(\\mathbf{x})) = A(\\mathbf{x} \\cdot \\mathbf{w} + b)$.\n",
    "\n",
    "## Predicting outputs\n",
    "\n",
    "With our set of weights and our $f(x)$, we are ready to begin predicting outputs for a set of inputs--inferencing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               predicted output\n",
      "observation #                  \n",
      "0                      0.010692\n",
      "1                      0.995758\n",
      "2                      0.995757\n",
      "3                      1.000000\n"
     ]
    }
   ],
   "source": [
    "predicted_output = sigmoid(numpy.dot(x, weights) + bias)\n",
    "predicted_output = pandas.DataFrame(\n",
    "    predicted_output,\n",
    "    columns=[\"predicted output\"],\n",
    "    index=inputs.index)\n",
    "\n",
    "print(predicted_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we strap this on to our truth table to see how we did compared to the true outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               input 1  input 2  true output  predicted output\n",
      "observation #                                                 \n",
      "0                    0        0            0          0.010692\n",
      "1                    0        1            1          0.995758\n",
      "2                    1        0            1          0.995757\n",
      "3                    1        1            1          1.000000\n"
     ]
    }
   ],
   "source": [
    "print(pandas.concat((\n",
    "    inputs,\n",
    "    ground_truth,\n",
    "    predicted_output),\n",
    "    axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing our model in PyTorch\n",
    "\n",
    "We implemented the above linear model using numpy and some hand-calculated derivatives to iterate through the gradient descent algorithm.  Higher-level frameworks like PyTorch simplify this process by providing canned versions of these operations that allow us to\n",
    "\n",
    "1. more quickly swap algorithms and functions in and out to see the effects of changing our activation function, loss function, and other hyperparameters\n",
    "2. automatically calculate derivatives of our loss function with respect to different variables for us, obviating the need to do pen-and-paper calculus to derive the correct values to feed into gradient descent\n",
    "3. easily offload these processes to accelerators like GPUs with only a few extra lines of code\n",
    "\n",
    "Here, let's use the PyTorch framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll implement the same function which is a linear function at its core:\n",
    "\n",
    "$ y(\\mathbf{x}) = \\mathbf{x} \\cdot \\mathbf{w} + b $\n",
    "\n",
    "which is then wrapped in our sigmoid function:\n",
    "\n",
    "$ A(x) = \\frac{1}{1 + e^{-x}} $\n",
    "\n",
    "which means\n",
    "\n",
    "$ f(x) = A(y(x)) $\n",
    "\n",
    "Instead of defining Python functions for $y(x)$ and $A(y)$ (as we did above with `linear(x, weights, bias)` and `sigmoid(x)`, we can assemble the model using PyTorch using canned versions of these functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(inputs.shape[1], 1),\n",
    "    torch.nn.Sigmoid()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `torch.nn.Sequential` _container_ to define a model that simply feeds the output of the first function (`Linear()`) into the second (`Sigmoid()`) to produce $A(y(x))$.\n",
    "\n",
    "Our $y(x)$ linear model is implemented in PyTorch as `torch.nn.Linear(num_inputs, num_outputs)`.\n",
    "\n",
    "Similarly, the sigmoid function is provided by PyTorch as `torch.nn.Sigmoid()`.  We can visually confirm that the two functions produce the same output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f03e1e908>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = numpy.arange(-10, 10, 0.1)\n",
    "matplotlib.pyplot.plot(x, torch.nn.Sigmoid()(torch.from_numpy(x)).detach().numpy(), label=\"PyTorch Sigmoid\")\n",
    "matplotlib.pyplot.scatter(x[::10], sigmoid(x[::10]), color='C1', label=\"our Sigmoid()\")\n",
    "matplotlib.pyplot.grid()\n",
    "matplotlib.pyplot.legend()\n",
    "matplotlib.pyplot.title(\"torch.nn.Sigmoid\")\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as before, we also set initial weights and biases for the linear part of our model.  This isn't strictly necessary with PyTorch since it will automatically initialize them to random values, but we initialize our PyTorch model with the same weights and biases as our hand-rolled version so that we can compare the gradient descent processes and make sure we are getting numerically identical results and behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting weights: tensor([0.4170, 0.7203], dtype=torch.float64, requires_grad=True)\n",
      "Starting bias: tensor([0.0001], dtype=torch.float64, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "numpy.random.seed(seed=1) # use same initial seed as before\n",
    "with torch.no_grad():\n",
    "    # torch.rand() is faster, but we use numpy to re-use the same random starting weights/biases\n",
    "    model[0].weight = torch.nn.Parameter(torch.from_numpy(numpy.random.rand(1, inputs.shape[1])))\n",
    "    print(\"Starting weights: {}\".format(model[0].weight.flatten()))\n",
    "    model[0].bias = torch.nn.Parameter(torch.from_numpy(numpy.random.rand(1, 1)))\n",
    "    print(\"Starting bias: {}\".format(model[0].bias.flatten()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our model defined and our initial weights and biases set, we can apply gradient descent.  A couple of notes:\n",
    "\n",
    "1. We have to convert our numpy array into PyTorch tensors.  They're functionally equivalent, but tensors have extra magic that allows PyTorch to track enough information about them to calculate gradients of them on demand.\n",
    "\n",
    "2. Our simple loss function is provided by PyTorch as `torch.nn.L1Loss()` which we provide here.\n",
    "\n",
    "3. We use `torch.optim.SGD()` as our _optimizer_ - this implements gradient descent as defined earlier ($ w_{t+1} = w_{t} - R \\cdot \\frac{d}{dw}E(x, w_{t}) $).  It actually implements _stochastic gradient descent_, but we aren't using the extra SGD parameters, and we are feeding it all of our inputs at once which renders it numerically identical to standard gradient descent.\n",
    "\n",
    "4. By using PyTorch tensors as inputs and ouputs to our loss function, we get not only the error as calculated based on the weights and bias at each step, but we can call the `backward()` method on that error tensor to calculate the derivatives of this loss function with respect to the weights and biases.  We don't have to do any calculus by hand!\n",
    "\n",
    "This last point is probably the most important thing PyTorch gives us to simplify the process of training model by implicitly tracking everything needed to calculate the gradient.  This has the downside that PyTorch is implicitly modifying variables behind the scenes without explicitly telling us; specifically, `backward()` not only calculates gradients, it updates parts of our model to store those gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error at step     0:   1.47e+00\n",
      "error at step  1000:   2.33e-01\n",
      "error at step  2000:   1.09e-01\n",
      "error at step  3000:   6.98e-02\n",
      "error at step  4000:   5.10e-02\n",
      "error at step  5000:   4.00e-02\n",
      "error at step  6000:   3.29e-02\n",
      "error at step  7000:   2.79e-02\n",
      "error at step  8000:   2.43e-02\n",
      "error at step  9000:   2.14e-02\n",
      "Final weights: [9.98578506 9.98589032]\n",
      "Final bias:    -4.52752860008788\n",
      "10000 iterations took 8.6 seconds\n"
     ]
    }
   ],
   "source": [
    "# pytorch is more sensitive about types, so we explicitly cast our integer input arrays as float64\n",
    "inputs_tensor = torch.from_numpy(inputs.to_numpy(dtype=numpy.float64))\n",
    "truth_tensor = torch.from_numpy(ground_truth.to_numpy(dtype=numpy.float64).reshape(-1, 1))\n",
    "\n",
    "# use PyTorch's implementation of our loss = abs(f0 - f)\n",
    "loss = torch.nn.L1Loss(reduction='sum')\n",
    "\n",
    "# use PyTorch's implementation of gradient descent\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "model.train()\n",
    "x = inputs_tensor\n",
    "t0 = time.time()\n",
    "for i in range(NUM_ITERATIONS):\n",
    "    f = model(x)\n",
    "\n",
    "    error = loss(f, truth_tensor)\n",
    "\n",
    "    # erase memory of gradients at previous step\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # compute gradient of loss function\n",
    "    error.backward()\n",
    "\n",
    "    # use computed gradient to update weights and bias\n",
    "    optimizer.step()\n",
    "\n",
    "    if i % (NUM_ITERATIONS / 10) == 0:\n",
    "        print(\"error at step {:5d}: {:10.2e}\".format(i, error.sum()))\n",
    "\n",
    "print(\"Final weights: {}\".format(next(model.parameters()).detach().numpy().flatten()))\n",
    "print(\"Final bias:    {}\".format(list(model.parameters())[-1].item()))\n",
    "print(\"{:d} iterations took {:.1f} seconds\".format(NUM_ITERATIONS, time.time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the implicit changes triggered by this _autograd_ functionality, we have to be _explicit_ when we don't want it to kick in by wrapping such tensor operations in `with torch.no_grad():`.  PyTorch's builtin gradient descent \"optimizer\" that we use takes care of this for us in this case.  Exactly how PyTorch's _autograd_ functionality works is illustrated very well in [A Gentle Introduction to torch.autograd](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html).\n",
    "\n",
    "Relatedly, we also explicitly set the model to be in training mode (`model.train()`) before training, then turn it off before inference to prevent inference from unintentionally changing our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=2, out_features=1, bias=True)\n",
       "  (1): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with our trained model, we can begin inferencing just as we did before, and we see that our predicted outputs are identical to our hand-rolled model before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               input 1  input 2  true output  predicted output\n",
      "observation #                                                 \n",
      "0                    0        0            0          0.010692\n",
      "1                    0        1            1          0.995758\n",
      "2                    1        0            1          0.995757\n",
      "3                    1        1            1          1.000000\n"
     ]
    }
   ],
   "source": [
    "predicted_output = model(inputs_tensor).detach().numpy()\n",
    "predicted_output = pandas.DataFrame(\n",
    "    predicted_output,\n",
    "    columns=[\"predicted output\"],\n",
    "    index=inputs.index)\n",
    "\n",
    "print(pandas.concat((\n",
    "    inputs,\n",
    "    ground_truth,\n",
    "    predicted_output),\n",
    "    axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Training on a GPU\n",
    "\n",
    "Our PyTorch implementation took a _lot_ longer than our hand-rolled implementation because the relative cost of training and evaluating our model was low compared to the overheads of PyTorch's automatic differentiation.  However PyTorch also greatly simplifies porting models to a GPU.  If you have a GPU, you can try running our PyTorch version of the model on it by adding a few extra lines.\n",
    "\n",
    "First define our GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing special as far as initializing weights and defining our loss and optimizer functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting weights: tensor([0.4170, 0.7203], dtype=torch.float64, requires_grad=True)\n",
      "Starting bias: tensor([0.0001], dtype=torch.float64, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# reinitialize weights\n",
    "numpy.random.seed(seed=1)\n",
    "with torch.no_grad():\n",
    "    model[0].weight = torch.nn.Parameter(torch.from_numpy(numpy.random.rand(1, inputs.shape[1])))\n",
    "    print(\"Starting weights: {}\".format(model[0].weight.flatten()))\n",
    "    model[0].bias = torch.nn.Parameter(torch.from_numpy(numpy.random.rand(1, 1)))\n",
    "    print(\"Starting bias: {}\".format(model[0].bias.flatten()))\n",
    "\n",
    "# define our loss function and optimizer\n",
    "loss = torch.nn.L1Loss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just have to make sure to send our model and input tensors to the GPU before we begin.  Everything else remains as it was, and all calls to our model will execute on the GPU since that's where we copied it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error at step     0:   1.47e+00\n",
      "error at step  1000:   2.33e-01\n",
      "error at step  2000:   1.09e-01\n",
      "error at step  3000:   6.98e-02\n",
      "error at step  4000:   5.10e-02\n",
      "error at step  5000:   4.00e-02\n",
      "error at step  6000:   3.29e-02\n",
      "error at step  7000:   2.79e-02\n",
      "error at step  8000:   2.43e-02\n",
      "error at step  9000:   2.14e-02\n",
      "Final weights: [9.98578506 9.98589032]\n",
      "Final bias:    -4.52752860008788\n",
      "10000 iterations took 23.1 seconds\n"
     ]
    }
   ],
   "source": [
    "# send everything to the GPU!\n",
    "model = model.to(device)\n",
    "inputs_tensor = torch.from_numpy(inputs.to_numpy(dtype=numpy.float64)).to(device)\n",
    "truth_tensor = torch.from_numpy(ground_truth.to_numpy(dtype=numpy.float64).reshape(-1, 1)).to(device)\n",
    "\n",
    "model.train()\n",
    "x = inputs_tensor\n",
    "t0 = time.time()\n",
    "for i in range(NUM_ITERATIONS):\n",
    "    f = model(x)\n",
    "    error = loss(f, truth_tensor)\n",
    "    optimizer.zero_grad()\n",
    "    error.backward()\n",
    "    optimizer.step()\n",
    "    if i % (NUM_ITERATIONS / 10) == 0:\n",
    "        print(\"error at step {:5d}: {:10.2e}\".format(i, error.sum()))\n",
    "\n",
    "# get our model back from the GPU so we can manipulate and print it from the CPU\n",
    "model = model.cpu()\n",
    "print(\"Final weights: {}\".format(next(model.parameters()).detach().numpy().flatten()))\n",
    "print(\"Final bias:    {}\".format(list(model.parameters())[-1].item()))\n",
    "print(\"{:d} iterations took {:.1f} seconds\".format(NUM_ITERATIONS, time.time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice that this took _even longer_ on the GPU--again, because this is such an algorithmically simple problem, the high-throughput capability of the GPU isn't able to shine."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "venv",
   "language": "python",
   "display_name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
