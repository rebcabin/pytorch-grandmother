{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "!pip install torch\n",
    "!pip install numpy\n",
    "!pip install torchvision\n",
    "!pip install matplotlib"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d6ecf7fe210b8efc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import warnings\n",
    "\n",
    "gpu_ok = False\n",
    "if torch.cuda.is_available():\n",
    "    device_cap = torch.cuda.get_device_capability()\n",
    "    if device_cap in ((7, 0), (8, 0), (9, 0)):\n",
    "        gpu_ok = True\n",
    "\n",
    "if not gpu_ok:\n",
    "    warnings.warn(\n",
    "        \"GPU is not NVIDIA V100, A100, or H100. Speedup numbers may be lower \"\n",
    "        \"than expected.\"\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ebbf67bb212eb2f1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "conda config --set solver classic\n",
    "conda install llvm-openmp"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fa7bbcba41edaa2d"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-4.6120e-01,  5.1593e-01, -9.5522e-02, -1.3514e-01,  6.2276e-01,\n",
      "          1.2636e+00,  8.9612e-01,  4.8567e-02,  1.8387e+00,  8.4815e-01],\n",
      "        [ 1.8079e-01,  1.6883e+00,  1.2309e+00,  3.0321e-02,  4.0621e-04,\n",
      "          5.2906e-01, -2.7183e-01,  3.5115e-01, -2.6703e-01,  1.7094e-01],\n",
      "        [ 1.4077e+00,  9.8121e-01,  1.5103e+00,  3.1748e-01, -9.0426e-01,\n",
      "          1.6459e+00, -7.6107e-01,  1.9974e-01,  2.6517e-02, -4.0693e-01],\n",
      "        [ 1.7764e-01, -1.2848e-01,  1.4828e+00,  1.1603e+00,  1.9190e+00,\n",
      "          1.9394e-01, -1.6496e+00,  1.8381e+00,  1.9387e+00,  1.3025e+00],\n",
      "        [-1.8597e-01,  1.2444e+00,  1.5192e+00,  1.5633e+00, -2.4584e-01,\n",
      "          1.0399e+00,  7.9803e-01,  1.2996e+00,  1.1561e+00,  1.9837e+00],\n",
      "        [ 1.5857e+00,  1.2321e+00,  7.5291e-02,  1.2942e+00, -1.7454e-01,\n",
      "          4.4495e-01,  1.6991e+00,  5.2596e-01, -2.9548e-01,  1.7128e+00],\n",
      "        [ 4.4707e-01,  1.4996e+00,  1.9653e+00, -8.3987e-01,  1.2762e+00,\n",
      "         -4.0492e-02, -3.7540e-01,  4.2987e-01,  3.0095e-01,  7.6592e-01],\n",
      "        [-6.5700e-01,  5.0684e-01,  3.3213e-01, -2.2524e-01,  3.3157e-01,\n",
      "         -2.0368e-02,  1.3744e-01,  7.6005e-01, -4.6533e-01,  1.9793e+00],\n",
      "        [ 7.5801e-01, -2.5521e-01, -1.4420e-01,  1.3933e+00,  4.1052e-01,\n",
      "          7.7723e-01,  7.2298e-01,  2.6950e-01,  1.5772e+00,  1.0104e+00],\n",
      "        [ 2.6178e-01, -1.9987e-01,  7.9377e-01,  1.9948e+00,  3.2021e-01,\n",
      "          1.1048e+00,  7.1507e-02,  1.4534e+00,  1.1404e+00, -6.3054e-01]])\n"
     ]
    }
   ],
   "source": [
    "def foo(x, y):\n",
    "    a = torch.sin(x)\n",
    "    b = torch.cos(y)\n",
    "    return a + b\n",
    "opt_foo1 = torch.compile(foo)\n",
    "print(opt_foo1(torch.randn(10, 10), torch.randn(10, 10)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T22:39:33.092777Z",
     "start_time": "2023-11-06T22:39:33.070720Z"
    }
   },
   "id": "f95416a04fefdc84"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.7327, -0.1781,  0.8547,  0.5238,  1.8608, -0.2566,  0.9199,  0.7763,\n",
      "          0.4317,  1.1656],\n",
      "        [ 1.5951,  0.4215, -0.7878,  0.4710,  0.3101, -0.7412,  1.1629,  0.1342,\n",
      "          1.1337,  0.8246],\n",
      "        [ 0.2285,  1.3989,  0.7062,  1.2177,  0.4093,  0.9948,  1.2976,  0.5116,\n",
      "          0.5670,  1.7790],\n",
      "        [-0.0395,  0.3823,  1.0397,  1.2731, -1.1471, -0.7769,  0.1530,  1.6605,\n",
      "          1.6583, -0.3554],\n",
      "        [-0.1727,  0.6536,  1.1635,  0.5863,  0.3256, -0.3476,  0.5461,  1.8593,\n",
      "          1.6389, -0.9585],\n",
      "        [ 1.2674,  1.5777,  1.9461,  1.6342,  0.4208,  1.8213,  0.5051,  0.0033,\n",
      "          1.3762,  0.2389],\n",
      "        [ 0.6999,  1.5613,  0.2678,  1.5436, -0.1075,  0.3879, -0.4156,  1.0373,\n",
      "          0.5513,  0.3663],\n",
      "        [ 0.1473,  1.5258,  0.4107, -0.2901,  1.0686,  0.2014,  1.1186,  0.0024,\n",
      "          1.6332, -0.5960],\n",
      "        [ 0.9158,  0.3434, -0.1527,  0.6057,  1.6743,  0.1910, -0.4644,  1.3931,\n",
      "          0.5976, -0.3042],\n",
      "        [ 1.3573,  0.1698,  0.2616, -0.9726, -0.1088,  0.1821,  1.6576,  0.6283,\n",
      "          1.6083,  1.8932]])\n"
     ]
    }
   ],
   "source": [
    "@torch.compile\n",
    "def opt_foo2(x, y):\n",
    "    a = torch.sin(x)\n",
    "    b = torch.cos(y)\n",
    "    return a + b\n",
    "print(opt_foo2(torch.randn(10, 10), torch.randn(10, 10)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T22:41:04.419454Z",
     "start_time": "2023-11-06T22:41:04.386265Z"
    }
   },
   "id": "d83cc01b3c14c08b"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0000, 1.1201, -0.0000, -0.0000, 0.2742, 0.2115, 0.9822, -0.0000, -0.0000,\n",
      "         -0.0000],\n",
      "        [-0.0000, -0.0000, -0.0000, 0.6332, -0.0000, 0.4483, -0.0000, 0.5720, 0.6222,\n",
      "         0.2011],\n",
      "        [-0.0000, 0.6631, 0.7174, 0.4235, -0.0000, -0.0000, 0.6718, -0.0000, -0.0000,\n",
      "         0.1483],\n",
      "        [0.2121, 0.5646, -0.0000, -0.0000, -0.0000, -0.0000, 0.4997, 0.3887, 0.5302,\n",
      "         -0.0000],\n",
      "        [-0.0000, 0.4428, 0.9691, 0.4156, -0.0000, 0.6691, -0.0000, -0.0000, 0.0518,\n",
      "         1.0681],\n",
      "        [0.1309, 0.0274, 0.7496, 0.1260, 0.0105, 0.5277, 0.4618, -0.0000, 0.0843,\n",
      "         -0.0000],\n",
      "        [0.2860, -0.0000, -0.0000, 0.1351, -0.0000, -0.0000, 1.0185, -0.0000, 0.4356,\n",
      "         0.1168],\n",
      "        [0.4358, -0.0000, 0.5472, 0.2373, -0.0000, -0.0000, 0.2919, -0.0000, 0.4593,\n",
      "         0.7429],\n",
      "        [-0.0000, 0.3665, -0.0000, 0.4963, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000],\n",
      "        [-0.0000, -0.0000, 0.3931, 0.1560, 0.0980, 0.8002, -0.0000, -0.0000, 0.1658,\n",
      "         0.2299]], grad_fn=<CompiledFunctionBackward>)\n"
     ]
    }
   ],
   "source": [
    "class MyModule(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin = torch.nn.Linear(100, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.nn.functional.relu(self.lin(x))\n",
    "\n",
    "mod = MyModule()\n",
    "opt_mod = torch.compile(mod)\n",
    "print(opt_mod(torch.randn(10, 100)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T22:41:31.544264Z",
     "start_time": "2023-11-06T22:41:30.537182Z"
    }
   },
   "id": "a760f6a4cd0a7a9d"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# Returns the result of running `fn()` and the time it took for `fn()` to run,\n",
    "# in seconds. We use CUDA events and synchronization for the most accurate\n",
    "# measurements.\n",
    "def timed(fn):\n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end = torch.cuda.Event(enable_timing=True)\n",
    "    start.record()\n",
    "    result = fn()\n",
    "    end.record()\n",
    "    torch.cuda.synchronize()\n",
    "    return result, start.elapsed_time(end) / 1000\n",
    "\n",
    "# Generates random input and targets data for the model, where `b` is\n",
    "# batch size.\n",
    "def generate_data(b):\n",
    "    return (\n",
    "        torch.randn(b, 3, 128, 128).to(torch.float32).cuda(),\n",
    "        torch.randint(1000, (b,)).cuda(),\n",
    "    )\n",
    "\n",
    "N_ITERS = 10\n",
    "\n",
    "from torchvision.models import densenet121\n",
    "def init_model():\n",
    "    return densenet121().to(torch.float32).cuda()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T22:42:15.346348Z",
     "start_time": "2023-11-06T22:42:15.237906Z"
    }
   },
   "id": "e53aeba0c27ed1d4"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[11], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43minit_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Reset since we are using a different mode.\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_dynamo\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[10], line 25\u001B[0m, in \u001B[0;36minit_model\u001B[0;34m()\u001B[0m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minit_model\u001B[39m():\n\u001B[0;32m---> 25\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mdensenet121\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat32\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcuda\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/mambaforge/envs/pytorch-compile-condavenv-3-10/lib/python3.10/site-packages/torch/nn/modules/module.py:918\u001B[0m, in \u001B[0;36mModule.cuda\u001B[0;34m(self, device)\u001B[0m\n\u001B[1;32m    901\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcuda\u001B[39m(\u001B[38;5;28mself\u001B[39m: T, device: Optional[Union[\u001B[38;5;28mint\u001B[39m, device]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m T:\n\u001B[1;32m    902\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Moves all model parameters and buffers to the GPU.\u001B[39;00m\n\u001B[1;32m    903\u001B[0m \n\u001B[1;32m    904\u001B[0m \u001B[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    916\u001B[0m \u001B[38;5;124;03m        Module: self\u001B[39;00m\n\u001B[1;32m    917\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 918\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcuda\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/mambaforge/envs/pytorch-compile-condavenv-3-10/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn, recurse)\u001B[0m\n\u001B[1;32m    808\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m recurse:\n\u001B[1;32m    809\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[0;32m--> 810\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    812\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[1;32m    813\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[1;32m    814\u001B[0m         \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[1;32m    815\u001B[0m         \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    820\u001B[0m         \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[1;32m    821\u001B[0m         \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "File \u001B[0;32m~/mambaforge/envs/pytorch-compile-condavenv-3-10/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn, recurse)\u001B[0m\n\u001B[1;32m    808\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m recurse:\n\u001B[1;32m    809\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[0;32m--> 810\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    812\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[1;32m    813\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[1;32m    814\u001B[0m         \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[1;32m    815\u001B[0m         \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    820\u001B[0m         \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[1;32m    821\u001B[0m         \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "File \u001B[0;32m~/mambaforge/envs/pytorch-compile-condavenv-3-10/lib/python3.10/site-packages/torch/nn/modules/module.py:833\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn, recurse)\u001B[0m\n\u001B[1;32m    829\u001B[0m \u001B[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001B[39;00m\n\u001B[1;32m    830\u001B[0m \u001B[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001B[39;00m\n\u001B[1;32m    831\u001B[0m \u001B[38;5;66;03m# `with torch.no_grad():`\u001B[39;00m\n\u001B[1;32m    832\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m--> 833\u001B[0m     param_applied \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparam\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    834\u001B[0m should_use_set_data \u001B[38;5;241m=\u001B[39m compute_should_use_set_data(param, param_applied)\n\u001B[1;32m    835\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m should_use_set_data:\n",
      "File \u001B[0;32m~/mambaforge/envs/pytorch-compile-condavenv-3-10/lib/python3.10/site-packages/torch/nn/modules/module.py:918\u001B[0m, in \u001B[0;36mModule.cuda.<locals>.<lambda>\u001B[0;34m(t)\u001B[0m\n\u001B[1;32m    901\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcuda\u001B[39m(\u001B[38;5;28mself\u001B[39m: T, device: Optional[Union[\u001B[38;5;28mint\u001B[39m, device]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m T:\n\u001B[1;32m    902\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Moves all model parameters and buffers to the GPU.\u001B[39;00m\n\u001B[1;32m    903\u001B[0m \n\u001B[1;32m    904\u001B[0m \u001B[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    916\u001B[0m \u001B[38;5;124;03m        Module: self\u001B[39;00m\n\u001B[1;32m    917\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 918\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_apply(\u001B[38;5;28;01mlambda\u001B[39;00m t: \u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcuda\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[0;32m~/mambaforge/envs/pytorch-compile-condavenv-3-10/lib/python3.10/site-packages/torch/cuda/__init__.py:289\u001B[0m, in \u001B[0;36m_lazy_init\u001B[0;34m()\u001B[0m\n\u001B[1;32m    284\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    285\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    286\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmultiprocessing, you must use the \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mspawn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m start method\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    287\u001B[0m     )\n\u001B[1;32m    288\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(torch\u001B[38;5;241m.\u001B[39m_C, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_cuda_getDeviceCount\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m--> 289\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTorch not compiled with CUDA enabled\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    290\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _cudart \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    291\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\n\u001B[1;32m    292\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    293\u001B[0m     )\n",
      "\u001B[0;31mAssertionError\u001B[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "model = init_model()\n",
    "\n",
    "# Reset since we are using a different mode.\n",
    "import torch._dynamo\n",
    "torch._dynamo.reset()\n",
    "\n",
    "model_opt = torch.compile(model, mode=\"reduce-overhead\")\n",
    "\n",
    "inp = generate_data(16)[0]\n",
    "with torch.no_grad():\n",
    "    print(\"eager:\", timed(lambda: model(inp))[1])\n",
    "    print(\"compile:\", timed(lambda: model_opt(inp))[1])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T22:42:37.644720Z",
     "start_time": "2023-11-06T22:42:37.242648Z"
    }
   },
   "id": "588fa2b9ade40dff"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[12], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m eager_times \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(N_ITERS):\n\u001B[0;32m----> 3\u001B[0m     inp \u001B[38;5;241m=\u001B[39m \u001B[43mgenerate_data\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m16\u001B[39;49m\u001B[43m)\u001B[49m[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m      4\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m      5\u001B[0m         _, eager_time \u001B[38;5;241m=\u001B[39m timed(\u001B[38;5;28;01mlambda\u001B[39;00m: model(inp))\n",
      "Cell \u001B[0;32mIn[10], line 17\u001B[0m, in \u001B[0;36mgenerate_data\u001B[0;34m(b)\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mgenerate_data\u001B[39m(b):\n\u001B[1;32m     16\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (\n\u001B[0;32m---> 17\u001B[0m         \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrandn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m128\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m128\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat32\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcuda\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m,\n\u001B[1;32m     18\u001B[0m         torch\u001B[38;5;241m.\u001B[39mrandint(\u001B[38;5;241m1000\u001B[39m, (b,))\u001B[38;5;241m.\u001B[39mcuda(),\n\u001B[1;32m     19\u001B[0m     )\n",
      "File \u001B[0;32m~/mambaforge/envs/pytorch-compile-condavenv-3-10/lib/python3.10/site-packages/torch/cuda/__init__.py:289\u001B[0m, in \u001B[0;36m_lazy_init\u001B[0;34m()\u001B[0m\n\u001B[1;32m    284\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    285\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    286\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmultiprocessing, you must use the \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mspawn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m start method\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    287\u001B[0m     )\n\u001B[1;32m    288\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(torch\u001B[38;5;241m.\u001B[39m_C, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_cuda_getDeviceCount\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m--> 289\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTorch not compiled with CUDA enabled\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    290\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _cudart \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    291\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\n\u001B[1;32m    292\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    293\u001B[0m     )\n",
      "\u001B[0;31mAssertionError\u001B[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "eager_times = []\n",
    "for i in range(N_ITERS):\n",
    "    inp = generate_data(16)[0]\n",
    "    with torch.no_grad():\n",
    "        _, eager_time = timed(lambda: model(inp))\n",
    "    eager_times.append(eager_time)\n",
    "    print(f\"eager eval time {i}: {eager_time}\")\n",
    "\n",
    "print(\"~\" * 10)\n",
    "\n",
    "compile_times = []\n",
    "for i in range(N_ITERS):\n",
    "    inp = generate_data(16)[0]\n",
    "    with torch.no_grad():\n",
    "        _, compile_time = timed(lambda: model_opt(inp))\n",
    "    compile_times.append(compile_time)\n",
    "    print(f\"compile eval time {i}: {compile_time}\")\n",
    "print(\"~\" * 10)\n",
    "\n",
    "import numpy as np\n",
    "eager_med = np.median(eager_times)\n",
    "compile_med = np.median(compile_times)\n",
    "speedup = eager_med / compile_med\n",
    "assert(speedup > 1)\n",
    "print(f\"(eval) eager median: {eager_med}, compile median: {compile_med}, speedup: {speedup}x\")\n",
    "print(\"~\" * 10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T22:43:27.398725Z",
     "start_time": "2023-11-06T22:43:27.359502Z"
    }
   },
   "id": "4ce203f16d834706"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[13], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43minit_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      2\u001B[0m opt \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39moptim\u001B[38;5;241m.\u001B[39mAdam(model\u001B[38;5;241m.\u001B[39mparameters())\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtrain\u001B[39m(mod, data):\n",
      "Cell \u001B[0;32mIn[10], line 25\u001B[0m, in \u001B[0;36minit_model\u001B[0;34m()\u001B[0m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minit_model\u001B[39m():\n\u001B[0;32m---> 25\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mdensenet121\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat32\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcuda\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/mambaforge/envs/pytorch-compile-condavenv-3-10/lib/python3.10/site-packages/torch/nn/modules/module.py:918\u001B[0m, in \u001B[0;36mModule.cuda\u001B[0;34m(self, device)\u001B[0m\n\u001B[1;32m    901\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcuda\u001B[39m(\u001B[38;5;28mself\u001B[39m: T, device: Optional[Union[\u001B[38;5;28mint\u001B[39m, device]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m T:\n\u001B[1;32m    902\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Moves all model parameters and buffers to the GPU.\u001B[39;00m\n\u001B[1;32m    903\u001B[0m \n\u001B[1;32m    904\u001B[0m \u001B[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    916\u001B[0m \u001B[38;5;124;03m        Module: self\u001B[39;00m\n\u001B[1;32m    917\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 918\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcuda\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/mambaforge/envs/pytorch-compile-condavenv-3-10/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn, recurse)\u001B[0m\n\u001B[1;32m    808\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m recurse:\n\u001B[1;32m    809\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[0;32m--> 810\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    812\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[1;32m    813\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[1;32m    814\u001B[0m         \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[1;32m    815\u001B[0m         \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    820\u001B[0m         \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[1;32m    821\u001B[0m         \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "File \u001B[0;32m~/mambaforge/envs/pytorch-compile-condavenv-3-10/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn, recurse)\u001B[0m\n\u001B[1;32m    808\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m recurse:\n\u001B[1;32m    809\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[0;32m--> 810\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    812\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[1;32m    813\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[1;32m    814\u001B[0m         \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[1;32m    815\u001B[0m         \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    820\u001B[0m         \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[1;32m    821\u001B[0m         \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "File \u001B[0;32m~/mambaforge/envs/pytorch-compile-condavenv-3-10/lib/python3.10/site-packages/torch/nn/modules/module.py:833\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn, recurse)\u001B[0m\n\u001B[1;32m    829\u001B[0m \u001B[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001B[39;00m\n\u001B[1;32m    830\u001B[0m \u001B[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001B[39;00m\n\u001B[1;32m    831\u001B[0m \u001B[38;5;66;03m# `with torch.no_grad():`\u001B[39;00m\n\u001B[1;32m    832\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m--> 833\u001B[0m     param_applied \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparam\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    834\u001B[0m should_use_set_data \u001B[38;5;241m=\u001B[39m compute_should_use_set_data(param, param_applied)\n\u001B[1;32m    835\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m should_use_set_data:\n",
      "File \u001B[0;32m~/mambaforge/envs/pytorch-compile-condavenv-3-10/lib/python3.10/site-packages/torch/nn/modules/module.py:918\u001B[0m, in \u001B[0;36mModule.cuda.<locals>.<lambda>\u001B[0;34m(t)\u001B[0m\n\u001B[1;32m    901\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcuda\u001B[39m(\u001B[38;5;28mself\u001B[39m: T, device: Optional[Union[\u001B[38;5;28mint\u001B[39m, device]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m T:\n\u001B[1;32m    902\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Moves all model parameters and buffers to the GPU.\u001B[39;00m\n\u001B[1;32m    903\u001B[0m \n\u001B[1;32m    904\u001B[0m \u001B[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    916\u001B[0m \u001B[38;5;124;03m        Module: self\u001B[39;00m\n\u001B[1;32m    917\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 918\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_apply(\u001B[38;5;28;01mlambda\u001B[39;00m t: \u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcuda\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[0;32m~/mambaforge/envs/pytorch-compile-condavenv-3-10/lib/python3.10/site-packages/torch/cuda/__init__.py:289\u001B[0m, in \u001B[0;36m_lazy_init\u001B[0;34m()\u001B[0m\n\u001B[1;32m    284\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    285\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    286\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmultiprocessing, you must use the \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mspawn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m start method\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    287\u001B[0m     )\n\u001B[1;32m    288\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(torch\u001B[38;5;241m.\u001B[39m_C, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_cuda_getDeviceCount\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m--> 289\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTorch not compiled with CUDA enabled\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    290\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _cudart \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    291\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\n\u001B[1;32m    292\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    293\u001B[0m     )\n",
      "\u001B[0;31mAssertionError\u001B[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "model = init_model()\n",
    "opt = torch.optim.Adam(model.parameters())\n",
    "\n",
    "def train(mod, data):\n",
    "    opt.zero_grad(True)\n",
    "    pred = mod(data[0])\n",
    "    loss = torch.nn.CrossEntropyLoss()(pred, data[1])\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "eager_times = []\n",
    "for i in range(N_ITERS):\n",
    "    inp = generate_data(16)\n",
    "    _, eager_time = timed(lambda: train(model, inp))\n",
    "    eager_times.append(eager_time)\n",
    "    print(f\"eager train time {i}: {eager_time}\")\n",
    "print(\"~\" * 10)\n",
    "\n",
    "model = init_model()\n",
    "opt = torch.optim.Adam(model.parameters())\n",
    "train_opt = torch.compile(train, mode=\"reduce-overhead\")\n",
    "\n",
    "compile_times = []\n",
    "for i in range(N_ITERS):\n",
    "    inp = generate_data(16)\n",
    "    _, compile_time = timed(lambda: train_opt(model, inp))\n",
    "    compile_times.append(compile_time)\n",
    "    print(f\"compile train time {i}: {compile_time}\")\n",
    "print(\"~\" * 10)\n",
    "\n",
    "eager_med = np.median(eager_times)\n",
    "compile_med = np.median(compile_times)\n",
    "speedup = eager_med / compile_med\n",
    "assert(speedup > 1)\n",
    "print(f\"(train) eager median: {eager_med}, compile median: {compile_med}, speedup: {speedup}x\")\n",
    "print(\"~\" * 10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T22:43:48.322753Z",
     "start_time": "2023-11-06T22:43:48.177123Z"
    }
   },
   "id": "abecb4b657b12f95"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def f1(x, y):\n",
    "    if x.sum() < 0:\n",
    "        return -y\n",
    "    return y\n",
    "\n",
    "# Test that `fn1` and `fn2` return the same result, given\n",
    "# the same arguments `args`. Typically, `fn1` will be an eager function\n",
    "# while `fn2` will be a compiled function (torch.compile, TorchScript, or FX graph).\n",
    "def test_fns(fn1, fn2, args):\n",
    "    out1 = fn1(*args)\n",
    "    out2 = fn2(*args)\n",
    "    return torch.allclose(out1, out2)\n",
    "\n",
    "inp1 = torch.randn(5, 5)\n",
    "inp2 = torch.randn(5, 5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T22:44:11.780431Z",
     "start_time": "2023-11-06T22:44:11.767438Z"
    }
   },
   "id": "c70524a129bfb85a"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traced 1, 1: True\n",
      "traced 1, 2: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cp/df32wpxn1ps8dmn6w_tydwdr0000gn/T/ipykernel_26898/2481013000.py:2: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if x.sum() < 0:\n"
     ]
    }
   ],
   "source": [
    "traced_f1 = torch.jit.trace(f1, (inp1, inp2))\n",
    "print(\"traced 1, 1:\", test_fns(f1, traced_f1, (inp1, inp2)))\n",
    "print(\"traced 1, 2:\", test_fns(f1, traced_f1, (-inp1, inp2)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T22:44:25.314355Z",
     "start_time": "2023-11-06T22:44:25.302196Z"
    }
   },
   "id": "4f2e4d6e657c5fd8"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cp/df32wpxn1ps8dmn6w_tydwdr0000gn/T/ipykernel_26898/1440212074.py\", line 3, in <module>\n",
      "    torch.fx.symbolic_trace(f1)\n",
      "  File \"/Users/brian/mambaforge/envs/pytorch-compile-condavenv-3-10/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py\", line 1150, in symbolic_trace\n",
      "    graph = tracer.trace(root, concrete_args)\n",
      "  File \"/Users/brian/mambaforge/envs/pytorch-compile-condavenv-3-10/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 328, in _fn\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/Users/brian/mambaforge/envs/pytorch-compile-condavenv-3-10/lib/python3.10/site-packages/torch/_dynamo/external_utils.py\", line 17, in inner\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/Users/brian/mambaforge/envs/pytorch-compile-condavenv-3-10/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py\", line 817, in trace\n",
      "    (self.create_arg(fn(*args)),),\n",
      "  File \"/var/folders/cp/df32wpxn1ps8dmn6w_tydwdr0000gn/T/ipykernel_26898/2481013000.py\", line 2, in f1\n",
      "    if x.sum() < 0:\n",
      "  File \"/Users/brian/mambaforge/envs/pytorch-compile-condavenv-3-10/lib/python3.10/site-packages/torch/fx/proxy.py\", line 437, in __bool__\n",
      "    return self.tracer.to_bool(self)\n",
      "  File \"/Users/brian/mambaforge/envs/pytorch-compile-condavenv-3-10/lib/python3.10/site-packages/torch/fx/proxy.py\", line 300, in to_bool\n",
      "    raise TraceError('symbolically traced variables cannot be used as inputs to control flow')\n",
      "torch.fx.proxy.TraceError: symbolically traced variables cannot be used as inputs to control flow\n"
     ]
    }
   ],
   "source": [
    "import traceback as tb\n",
    "try:\n",
    "    torch.fx.symbolic_trace(f1)\n",
    "except:\n",
    "    tb.print_exc()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T22:44:56.153137Z",
     "start_time": "2023-11-06T22:44:56.139856Z"
    }
   },
   "id": "1f600f48b8554f37"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fx 1, 1: True\n",
      "fx 1, 2: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brian/mambaforge/envs/pytorch-compile-condavenv-3-10/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:634: UserWarning: Was not able to add assertion to guarantee correct input x to specialized function. It is up to the user to make sure that your inputs match the inputs you specialized the function with.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "fx_f1 = torch.fx.symbolic_trace(f1, concrete_args={\"x\": inp1})\n",
    "print(\"fx 1, 1:\", test_fns(f1, fx_f1, (inp1, inp2)))\n",
    "print(\"fx 1, 2:\", test_fns(f1, fx_f1, (-inp1, inp2)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T22:45:11.639578Z",
     "start_time": "2023-11-06T22:45:11.625811Z"
    }
   },
   "id": "52bcff373c36c78d"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compile 1, 1: True\n",
      "compile 1, 2: True\n",
      "~~~~~~~~~~\n"
     ]
    }
   ],
   "source": [
    "# Reset since we are using a different mode.\n",
    "torch._dynamo.reset()\n",
    "\n",
    "compile_f1 = torch.compile(f1)\n",
    "print(\"compile 1, 1:\", test_fns(f1, compile_f1, (inp1, inp2)))\n",
    "print(\"compile 1, 2:\", test_fns(f1, compile_f1, (-inp1, inp2)))\n",
    "print(\"~\" * 10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T22:45:31.737076Z",
     "start_time": "2023-11-06T22:45:30.343482Z"
    }
   },
   "id": "d66abe24a50d90e1"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cp/df32wpxn1ps8dmn6w_tydwdr0000gn/T/ipykernel_26898/3756610000.py\", line 9, in <module>\n",
      "    script_f2(inp1, inp2)\n",
      "RuntimeError: f2() Expected a value of type 'Tensor (inferred)' for argument 'y' but instead found type 'int'.\n",
      "Inferred 'y' to be of type 'Tensor' because it was not annotated with an explicit type.\n",
      "Position: 1\n",
      "Value: 3\n",
      "Declaration: f2(Tensor x, Tensor y) -> Tensor\n",
      "Cast error details: Unable to cast 3 to Tensor\n"
     ]
    }
   ],
   "source": [
    "def f2(x, y):\n",
    "    return x + y\n",
    "\n",
    "inp1 = torch.randn(5, 5)\n",
    "inp2 = 3\n",
    "\n",
    "script_f2 = torch.jit.script(f2)\n",
    "try:\n",
    "    script_f2(inp1, inp2)\n",
    "except:\n",
    "    tb.print_exc()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T22:45:47.890154Z",
     "start_time": "2023-11-06T22:45:47.875689Z"
    }
   },
   "id": "d2138ac2186f0e89"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compile 2: True\n",
      "~~~~~~~~~~\n"
     ]
    }
   ],
   "source": [
    "compile_f2 = torch.compile(f2)\n",
    "print(\"compile 2:\", test_fns(f2, compile_f2, (inp1, inp2)))\n",
    "print(\"~\" * 10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T22:46:09.145556Z",
     "start_time": "2023-11-06T22:46:08.413358Z"
    }
   },
   "id": "b788576a293bc8e0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "conda install scipy"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "81b9838dd173b5a3"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "import scipy\n",
    "def f3(x):\n",
    "    x = x * 2\n",
    "    x = scipy.fft.dct(x.numpy())\n",
    "    x = torch.from_numpy(x)\n",
    "    x = x * 2\n",
    "    return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T22:47:12.857815Z",
     "start_time": "2023-11-06T22:47:12.850731Z"
    }
   },
   "id": "9d2ed71328ae1529"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traced 3: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cp/df32wpxn1ps8dmn6w_tydwdr0000gn/T/ipykernel_26898/2375033046.py:4: TracerWarning: Converting a tensor to a NumPy array might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  x = scipy.fft.dct(x.numpy())\n",
      "/var/folders/cp/df32wpxn1ps8dmn6w_tydwdr0000gn/T/ipykernel_26898/2375033046.py:5: TracerWarning: torch.from_numpy results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  x = torch.from_numpy(x)\n"
     ]
    }
   ],
   "source": [
    "inp1 = torch.randn(5, 5)\n",
    "inp2 = torch.randn(5, 5)\n",
    "traced_f3 = torch.jit.trace(f3, (inp1,))\n",
    "print(\"traced 3:\", test_fns(f3, traced_f3, (inp2,)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T22:47:30.903940Z",
     "start_time": "2023-11-06T22:47:30.849042Z"
    }
   },
   "id": "4e021bf02262890e"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cp/df32wpxn1ps8dmn6w_tydwdr0000gn/T/ipykernel_26898/2673820167.py\", line 2, in <module>\n",
      "    torch.jit.script(f3)\n",
      "  File \"/Users/brian/mambaforge/envs/pytorch-compile-condavenv-3-10/lib/python3.10/site-packages/torch/jit/_script.py\", line 1381, in script\n",
      "    fn = torch._C._jit_script_compile(\n",
      "  File \"/Users/brian/mambaforge/envs/pytorch-compile-condavenv-3-10/lib/python3.10/site-packages/torch/_jit_internal.py\", line 1205, in _try_get_dispatched_fn\n",
      "    return boolean_dispatched.get(fn)\n",
      "  File \"/Users/brian/mambaforge/envs/pytorch-compile-condavenv-3-10/lib/python3.10/weakref.py\", line 453, in get\n",
      "    return self.data.get(ref(key),default)\n",
      "TypeError: cannot create weak reference to 'uarray._Function' object\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cp/df32wpxn1ps8dmn6w_tydwdr0000gn/T/ipykernel_26898/2673820167.py\", line 7, in <module>\n",
      "    torch.fx.symbolic_trace(f3)\n",
      "  File \"/Users/brian/mambaforge/envs/pytorch-compile-condavenv-3-10/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py\", line 1150, in symbolic_trace\n",
      "    graph = tracer.trace(root, concrete_args)\n",
      "  File \"/Users/brian/mambaforge/envs/pytorch-compile-condavenv-3-10/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 328, in _fn\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/Users/brian/mambaforge/envs/pytorch-compile-condavenv-3-10/lib/python3.10/site-packages/torch/_dynamo/external_utils.py\", line 17, in inner\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/Users/brian/mambaforge/envs/pytorch-compile-condavenv-3-10/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py\", line 817, in trace\n",
      "    (self.create_arg(fn(*args)),),\n",
      "  File \"/var/folders/cp/df32wpxn1ps8dmn6w_tydwdr0000gn/T/ipykernel_26898/2375033046.py\", line 4, in f3\n",
      "    x = scipy.fft.dct(x.numpy())\n",
      "  File \"/Users/brian/mambaforge/envs/pytorch-compile-condavenv-3-10/lib/python3.10/site-packages/scipy/fft/_backend.py\", line 25, in __ua_function__\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/Users/brian/mambaforge/envs/pytorch-compile-condavenv-3-10/lib/python3.10/site-packages/scipy/fft/_pocketfft/realtransforms.py\", line 19, in _r2r\n",
      "    tmp = _asfarray(x)\n",
      "  File \"/Users/brian/mambaforge/envs/pytorch-compile-condavenv-3-10/lib/python3.10/site-packages/scipy/fft/_pocketfft/helper.py\", line 89, in _asfarray\n",
      "    if x.dtype == np.float16:\n",
      "  File \"/Users/brian/mambaforge/envs/pytorch-compile-condavenv-3-10/lib/python3.10/site-packages/torch/fx/proxy.py\", line 542, in impl\n",
      "    return tracer.create_proxy('call_function', target, args, kwargs)\n",
      "  File \"/Users/brian/mambaforge/envs/pytorch-compile-condavenv-3-10/lib/python3.10/site-packages/torch/fx/proxy.py\", line 184, in create_proxy\n",
      "    args_ = self.create_arg(args)\n",
      "  File \"/Users/brian/mambaforge/envs/pytorch-compile-condavenv-3-10/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py\", line 385, in create_arg\n",
      "    return super().create_arg(a)\n",
      "  File \"/Users/brian/mambaforge/envs/pytorch-compile-condavenv-3-10/lib/python3.10/site-packages/torch/fx/proxy.py\", line 255, in create_arg\n",
      "    return type(a)(self.create_arg(elem) for elem in a)\n",
      "  File \"/Users/brian/mambaforge/envs/pytorch-compile-condavenv-3-10/lib/python3.10/site-packages/torch/fx/proxy.py\", line 255, in <genexpr>\n",
      "    return type(a)(self.create_arg(elem) for elem in a)\n",
      "  File \"/Users/brian/mambaforge/envs/pytorch-compile-condavenv-3-10/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py\", line 385, in create_arg\n",
      "    return super().create_arg(a)\n",
      "  File \"/Users/brian/mambaforge/envs/pytorch-compile-condavenv-3-10/lib/python3.10/site-packages/torch/fx/proxy.py\", line 291, in create_arg\n",
      "    raise NotImplementedError(f\"argument of type: {type(a)}\")\n",
      "NotImplementedError: argument of type: <class 'type'>\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    torch.jit.script(f3)\n",
    "except:\n",
    "    tb.print_exc()\n",
    "\n",
    "try:\n",
    "    torch.fx.symbolic_trace(f3)\n",
    "except:\n",
    "    tb.print_exc()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T22:47:47.856281Z",
     "start_time": "2023-11-06T22:47:47.840793Z"
    }
   },
   "id": "bfcd0a138c722046"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compile 3: True\n"
     ]
    }
   ],
   "source": [
    "compile_f3 = torch.compile(f3)\n",
    "print(\"compile 3:\", test_fns(f3, compile_f3, (inp2,)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T22:48:14.498005Z",
     "start_time": "2023-11-06T22:48:13.791115Z"
    }
   },
   "id": "e2b4322daa76ab2d"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[26], line 10\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# Reset since we are using a different backend.\u001B[39;00m\n\u001B[1;32m      8\u001B[0m torch\u001B[38;5;241m.\u001B[39m_dynamo\u001B[38;5;241m.\u001B[39mreset()\n\u001B[0;32m---> 10\u001B[0m opt_model \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcompile(\u001B[43minit_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m, backend\u001B[38;5;241m=\u001B[39mcustom_backend)\n\u001B[1;32m     11\u001B[0m opt_model(generate_data(\u001B[38;5;241m16\u001B[39m)[\u001B[38;5;241m0\u001B[39m])\n",
      "Cell \u001B[0;32mIn[10], line 25\u001B[0m, in \u001B[0;36minit_model\u001B[0;34m()\u001B[0m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minit_model\u001B[39m():\n\u001B[0;32m---> 25\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mdensenet121\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat32\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcuda\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/mambaforge/envs/pytorch-compile-condavenv-3-10/lib/python3.10/site-packages/torch/nn/modules/module.py:918\u001B[0m, in \u001B[0;36mModule.cuda\u001B[0;34m(self, device)\u001B[0m\n\u001B[1;32m    901\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcuda\u001B[39m(\u001B[38;5;28mself\u001B[39m: T, device: Optional[Union[\u001B[38;5;28mint\u001B[39m, device]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m T:\n\u001B[1;32m    902\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Moves all model parameters and buffers to the GPU.\u001B[39;00m\n\u001B[1;32m    903\u001B[0m \n\u001B[1;32m    904\u001B[0m \u001B[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    916\u001B[0m \u001B[38;5;124;03m        Module: self\u001B[39;00m\n\u001B[1;32m    917\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 918\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcuda\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/mambaforge/envs/pytorch-compile-condavenv-3-10/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn, recurse)\u001B[0m\n\u001B[1;32m    808\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m recurse:\n\u001B[1;32m    809\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[0;32m--> 810\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    812\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[1;32m    813\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[1;32m    814\u001B[0m         \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[1;32m    815\u001B[0m         \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    820\u001B[0m         \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[1;32m    821\u001B[0m         \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "File \u001B[0;32m~/mambaforge/envs/pytorch-compile-condavenv-3-10/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn, recurse)\u001B[0m\n\u001B[1;32m    808\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m recurse:\n\u001B[1;32m    809\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[0;32m--> 810\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    812\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[1;32m    813\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[1;32m    814\u001B[0m         \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[1;32m    815\u001B[0m         \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    820\u001B[0m         \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[1;32m    821\u001B[0m         \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "File \u001B[0;32m~/mambaforge/envs/pytorch-compile-condavenv-3-10/lib/python3.10/site-packages/torch/nn/modules/module.py:833\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn, recurse)\u001B[0m\n\u001B[1;32m    829\u001B[0m \u001B[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001B[39;00m\n\u001B[1;32m    830\u001B[0m \u001B[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001B[39;00m\n\u001B[1;32m    831\u001B[0m \u001B[38;5;66;03m# `with torch.no_grad():`\u001B[39;00m\n\u001B[1;32m    832\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m--> 833\u001B[0m     param_applied \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparam\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    834\u001B[0m should_use_set_data \u001B[38;5;241m=\u001B[39m compute_should_use_set_data(param, param_applied)\n\u001B[1;32m    835\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m should_use_set_data:\n",
      "File \u001B[0;32m~/mambaforge/envs/pytorch-compile-condavenv-3-10/lib/python3.10/site-packages/torch/nn/modules/module.py:918\u001B[0m, in \u001B[0;36mModule.cuda.<locals>.<lambda>\u001B[0;34m(t)\u001B[0m\n\u001B[1;32m    901\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcuda\u001B[39m(\u001B[38;5;28mself\u001B[39m: T, device: Optional[Union[\u001B[38;5;28mint\u001B[39m, device]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m T:\n\u001B[1;32m    902\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Moves all model parameters and buffers to the GPU.\u001B[39;00m\n\u001B[1;32m    903\u001B[0m \n\u001B[1;32m    904\u001B[0m \u001B[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    916\u001B[0m \u001B[38;5;124;03m        Module: self\u001B[39;00m\n\u001B[1;32m    917\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 918\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_apply(\u001B[38;5;28;01mlambda\u001B[39;00m t: \u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcuda\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[0;32m~/mambaforge/envs/pytorch-compile-condavenv-3-10/lib/python3.10/site-packages/torch/cuda/__init__.py:289\u001B[0m, in \u001B[0;36m_lazy_init\u001B[0;34m()\u001B[0m\n\u001B[1;32m    284\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    285\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    286\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmultiprocessing, you must use the \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mspawn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m start method\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    287\u001B[0m     )\n\u001B[1;32m    288\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(torch\u001B[38;5;241m.\u001B[39m_C, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_cuda_getDeviceCount\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m--> 289\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTorch not compiled with CUDA enabled\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    290\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _cudart \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    291\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\n\u001B[1;32m    292\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    293\u001B[0m     )\n",
      "\u001B[0;31mAssertionError\u001B[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "def custom_backend(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):\n",
    "    print(\"custom backend called with FX graph:\")\n",
    "    gm.graph.print_tabular()\n",
    "    return gm.forward\n",
    "\n",
    "# Reset since we are using a different backend.\n",
    "torch._dynamo.reset()\n",
    "\n",
    "opt_model = torch.compile(init_model(), backend=custom_backend)\n",
    "opt_model(generate_data(16)[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T22:48:29.381996Z",
     "start_time": "2023-11-06T22:48:29.226642Z"
    }
   },
   "id": "e7f30d361123ae57"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom backend called with FX graph:\n",
      "`print_tabular` relies on the library `tabulate`, which could not be found on this machine. Run `pip install tabulate` to install the library.\n"
     ]
    },
    {
     "ename": "BackendCompilerFailed",
     "evalue": "backend='custom_backend' raised:\nModuleNotFoundError: No module named 'tabulate'\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mBackendCompilerFailed\u001B[0m                     Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[27], line 10\u001B[0m\n\u001B[1;32m      8\u001B[0m inp1 \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mrandn(\u001B[38;5;241m10\u001B[39m)\n\u001B[1;32m      9\u001B[0m inp2 \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mrandn(\u001B[38;5;241m10\u001B[39m)\n\u001B[0;32m---> 10\u001B[0m \u001B[43mopt_bar\u001B[49m\u001B[43m(\u001B[49m\u001B[43minp1\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minp2\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     11\u001B[0m opt_bar(inp1, \u001B[38;5;241m-\u001B[39minp2)\n",
      "File \u001B[0;32m~/mambaforge/envs/pytorch-compile-condavenv-3-10/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:328\u001B[0m, in \u001B[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    326\u001B[0m dynamic_ctx\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__enter__\u001B[39m()\n\u001B[1;32m    327\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 328\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    330\u001B[0m     set_eval_frame(prior)\n",
      "File \u001B[0;32m~/mambaforge/envs/pytorch-compile-condavenv-3-10/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:490\u001B[0m, in \u001B[0;36mcatch_errors_wrapper.<locals>.catch_errors\u001B[0;34m(frame, cache_entry, frame_state)\u001B[0m\n\u001B[1;32m    487\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m hijacked_callback(frame, cache_entry, hooks, frame_state)\n\u001B[1;32m    489\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m compile_lock, _disable_current_modes():\n\u001B[0;32m--> 490\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcallback\u001B[49m\u001B[43m(\u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcache_entry\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhooks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe_state\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/mambaforge/envs/pytorch-compile-condavenv-3-10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:641\u001B[0m, in \u001B[0;36mconvert_frame.<locals>._convert_frame\u001B[0;34m(frame, cache_size, hooks, frame_state)\u001B[0m\n\u001B[1;32m    639\u001B[0m counters[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mframes\u001B[39m\u001B[38;5;124m\"\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtotal\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    640\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 641\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43minner_convert\u001B[49m\u001B[43m(\u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcache_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhooks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe_state\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    642\u001B[0m     counters[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mframes\u001B[39m\u001B[38;5;124m\"\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mok\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    643\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "File \u001B[0;32m~/mambaforge/envs/pytorch-compile-condavenv-3-10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:133\u001B[0m, in \u001B[0;36mwrap_convert_context.<locals>._fn\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    131\u001B[0m cleanup \u001B[38;5;241m=\u001B[39m setup_compile_debug()\n\u001B[1;32m    132\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 133\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    134\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    135\u001B[0m     cleanup\u001B[38;5;241m.\u001B[39mclose()\n",
      "File \u001B[0;32m~/mambaforge/envs/pytorch-compile-condavenv-3-10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:389\u001B[0m, in \u001B[0;36mconvert_frame_assert.<locals>._convert_frame_assert\u001B[0;34m(frame, cache_entry, hooks, frame_state)\u001B[0m\n\u001B[1;32m    376\u001B[0m compile_id \u001B[38;5;241m=\u001B[39m CompileId(frame_id, frame_compile_id)\n\u001B[1;32m    378\u001B[0m signpost_event(\n\u001B[1;32m    379\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdynamo\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    380\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_convert_frame_assert._compile\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    386\u001B[0m     },\n\u001B[1;32m    387\u001B[0m )\n\u001B[0;32m--> 389\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_compile\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    390\u001B[0m \u001B[43m    \u001B[49m\u001B[43mframe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mf_code\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    391\u001B[0m \u001B[43m    \u001B[49m\u001B[43mframe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mf_globals\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    392\u001B[0m \u001B[43m    \u001B[49m\u001B[43mframe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mf_locals\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    393\u001B[0m \u001B[43m    \u001B[49m\u001B[43mframe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mf_builtins\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    394\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompiler_fn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    395\u001B[0m \u001B[43m    \u001B[49m\u001B[43mone_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    396\u001B[0m \u001B[43m    \u001B[49m\u001B[43mexport\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    397\u001B[0m \u001B[43m    \u001B[49m\u001B[43mexport_constraints\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    398\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhooks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    399\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcache_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    400\u001B[0m \u001B[43m    \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    401\u001B[0m \u001B[43m    \u001B[49m\u001B[43mframe_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mframe_state\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    402\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompile_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcompile_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    403\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/mambaforge/envs/pytorch-compile-condavenv-3-10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:569\u001B[0m, in \u001B[0;36m_compile\u001B[0;34m(code, globals, locals, builtins, compiler_fn, one_graph, export, export_constraints, hooks, cache_size, frame, frame_state, compile_id)\u001B[0m\n\u001B[1;32m    567\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m compile_context(CompileContext(compile_id)):\n\u001B[1;32m    568\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 569\u001B[0m         guarded_code \u001B[38;5;241m=\u001B[39m \u001B[43mcompile_inner\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mone_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhooks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtransform\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    570\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m guarded_code\n\u001B[1;32m    571\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m (\n\u001B[1;32m    572\u001B[0m         Unsupported,\n\u001B[1;32m    573\u001B[0m         TorchRuntimeError,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    578\u001B[0m         ValidationException,\n\u001B[1;32m    579\u001B[0m     ) \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/mambaforge/envs/pytorch-compile-condavenv-3-10/lib/python3.10/site-packages/torch/_dynamo/utils.py:189\u001B[0m, in \u001B[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    187\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mrecord_function(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkey\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m (dynamo_timed)\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m    188\u001B[0m     t0 \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m--> 189\u001B[0m     r \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    190\u001B[0m     time_spent \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m-\u001B[39m t0\n\u001B[1;32m    191\u001B[0m compilation_time_metrics[key]\u001B[38;5;241m.\u001B[39mappend(time_spent)\n",
      "File \u001B[0;32m~/mambaforge/envs/pytorch-compile-condavenv-3-10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:491\u001B[0m, in \u001B[0;36m_compile.<locals>.compile_inner\u001B[0;34m(code, one_graph, hooks, transform)\u001B[0m\n\u001B[1;32m    489\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m attempt \u001B[38;5;129;01min\u001B[39;00m itertools\u001B[38;5;241m.\u001B[39mcount():\n\u001B[1;32m    490\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 491\u001B[0m         out_code \u001B[38;5;241m=\u001B[39m \u001B[43mtransform_code_object\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtransform\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    492\u001B[0m         orig_code_map[out_code] \u001B[38;5;241m=\u001B[39m code\n\u001B[1;32m    493\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[0;32m~/mambaforge/envs/pytorch-compile-condavenv-3-10/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py:1028\u001B[0m, in \u001B[0;36mtransform_code_object\u001B[0;34m(code, transformations, safe)\u001B[0m\n\u001B[1;32m   1025\u001B[0m instructions \u001B[38;5;241m=\u001B[39m cleaned_instructions(code, safe)\n\u001B[1;32m   1026\u001B[0m propagate_line_nums(instructions)\n\u001B[0;32m-> 1028\u001B[0m \u001B[43mtransformations\u001B[49m\u001B[43m(\u001B[49m\u001B[43minstructions\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcode_options\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1029\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m clean_and_assemble_instructions(instructions, keys, code_options)[\u001B[38;5;241m1\u001B[39m]\n",
      "File \u001B[0;32m~/mambaforge/envs/pytorch-compile-condavenv-3-10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:458\u001B[0m, in \u001B[0;36m_compile.<locals>.transform\u001B[0;34m(instructions, code_options)\u001B[0m\n\u001B[1;32m    456\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    457\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m tracing(tracer\u001B[38;5;241m.\u001B[39moutput\u001B[38;5;241m.\u001B[39mtracing_context):\n\u001B[0;32m--> 458\u001B[0m         \u001B[43mtracer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    459\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (exc\u001B[38;5;241m.\u001B[39mRestartAnalysis, exc\u001B[38;5;241m.\u001B[39mSkipFrame):\n\u001B[1;32m    460\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "File \u001B[0;32m~/mambaforge/envs/pytorch-compile-condavenv-3-10/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:2074\u001B[0m, in \u001B[0;36mInstructionTranslator.run\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   2073\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrun\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m-> 2074\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/mambaforge/envs/pytorch-compile-condavenv-3-10/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:724\u001B[0m, in \u001B[0;36mInstructionTranslatorBase.run\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    719\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    720\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput\u001B[38;5;241m.\u001B[39mpush_tx(\u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m    721\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m (\n\u001B[1;32m    722\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minstruction_pointer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    723\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput\u001B[38;5;241m.\u001B[39mshould_exit\n\u001B[0;32m--> 724\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    725\u001B[0m     ):\n\u001B[1;32m    726\u001B[0m         \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[1;32m    727\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m BackendCompilerFailed:\n",
      "File \u001B[0;32m~/mambaforge/envs/pytorch-compile-condavenv-3-10/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:688\u001B[0m, in \u001B[0;36mInstructionTranslatorBase.step\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    684\u001B[0m         unimplemented(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmissing: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00minst\u001B[38;5;241m.\u001B[39mopname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    685\u001B[0m     TracingContext\u001B[38;5;241m.\u001B[39mset_current_loc(\n\u001B[1;32m    686\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mf_code\u001B[38;5;241m.\u001B[39mco_filename, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlineno, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mf_code\u001B[38;5;241m.\u001B[39mco_name\n\u001B[1;32m    687\u001B[0m     )\n\u001B[0;32m--> 688\u001B[0m     \u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minst\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopname\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[43minst\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    690\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m inst\u001B[38;5;241m.\u001B[39mopname \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRETURN_VALUE\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    691\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m Unsupported:\n",
      "File \u001B[0;32m~/mambaforge/envs/pytorch-compile-condavenv-3-10/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:309\u001B[0m, in \u001B[0;36mgeneric_jump.<locals>.inner\u001B[0;34m(self, inst)\u001B[0m\n\u001B[1;32m    307\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpush(value)\n\u001B[1;32m    308\u001B[0m log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgeneric_jump triggered compile\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 309\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moutput\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompile_subgraph\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    310\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    311\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreason\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mGraphCompileReason\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    312\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mgeneric_jump \u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mtypestr\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mframe_summary\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\n\u001B[1;32m    313\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    314\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    315\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpop()\n\u001B[1;32m    317\u001B[0m if_next \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcreate_call_resume_at(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnext_instruction)\n",
      "File \u001B[0;32m~/mambaforge/envs/pytorch-compile-condavenv-3-10/lib/python3.10/site-packages/torch/_dynamo/output_graph.py:833\u001B[0m, in \u001B[0;36mOutputGraph.compile_subgraph\u001B[0;34m(self, tx, partial_convert, reason)\u001B[0m\n\u001B[1;32m    830\u001B[0m     append_prefix_insts()\n\u001B[1;32m    831\u001B[0m     \u001B[38;5;66;03m# optimization to generate better code in a common case\u001B[39;00m\n\u001B[1;32m    832\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39madd_output_instructions(\n\u001B[0;32m--> 833\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompile_and_call_fx_graph\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mreversed\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mstack_values\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mroot\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    834\u001B[0m         \u001B[38;5;241m+\u001B[39m [create_instruction(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUNPACK_SEQUENCE\u001B[39m\u001B[38;5;124m\"\u001B[39m, arg\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mlen\u001B[39m(stack_values))]\n\u001B[1;32m    835\u001B[0m     )\n\u001B[1;32m    836\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    837\u001B[0m     graph_output_var \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnew_var(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgraph_out\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/mambaforge/envs/pytorch-compile-condavenv-3-10/lib/python3.10/contextlib.py:79\u001B[0m, in \u001B[0;36mContextDecorator.__call__.<locals>.inner\u001B[0;34m(*args, **kwds)\u001B[0m\n\u001B[1;32m     76\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(func)\n\u001B[1;32m     77\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minner\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds):\n\u001B[1;32m     78\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_recreate_cm():\n\u001B[0;32m---> 79\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/mambaforge/envs/pytorch-compile-condavenv-3-10/lib/python3.10/site-packages/torch/_dynamo/output_graph.py:957\u001B[0m, in \u001B[0;36mOutputGraph.compile_and_call_fx_graph\u001B[0;34m(self, tx, rv, root)\u001B[0m\n\u001B[1;32m    952\u001B[0m graph_tabular_log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, lazy_format_graph_tabular(name, gm))\n\u001B[1;32m    953\u001B[0m graph_sizes_log\u001B[38;5;241m.\u001B[39mdebug(\n\u001B[1;32m    954\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, LazyString(\u001B[38;5;28;01mlambda\u001B[39;00m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_graph_sizes_log_str(name))\n\u001B[1;32m    955\u001B[0m )\n\u001B[0;32m--> 957\u001B[0m compiled_fn \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall_user_compiler\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgm\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    958\u001B[0m compiled_fn \u001B[38;5;241m=\u001B[39m disable(compiled_fn)\n\u001B[1;32m    960\u001B[0m counters[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstats\u001B[39m\u001B[38;5;124m\"\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124munique_graphs\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[0;32m~/mambaforge/envs/pytorch-compile-condavenv-3-10/lib/python3.10/site-packages/torch/_dynamo/utils.py:189\u001B[0m, in \u001B[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    187\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mrecord_function(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkey\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m (dynamo_timed)\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m    188\u001B[0m     t0 \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m--> 189\u001B[0m     r \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    190\u001B[0m     time_spent \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m-\u001B[39m t0\n\u001B[1;32m    191\u001B[0m compilation_time_metrics[key]\u001B[38;5;241m.\u001B[39mappend(time_spent)\n",
      "File \u001B[0;32m~/mambaforge/envs/pytorch-compile-condavenv-3-10/lib/python3.10/site-packages/torch/_dynamo/output_graph.py:1024\u001B[0m, in \u001B[0;36mOutputGraph.call_user_compiler\u001B[0;34m(self, gm)\u001B[0m\n\u001B[1;32m   1022\u001B[0m     unimplemented_with_warning(e, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mroot_tx\u001B[38;5;241m.\u001B[39mf_code, msg)\n\u001B[1;32m   1023\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m-> 1024\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m BackendCompilerFailed(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompiler_fn, e)\u001B[38;5;241m.\u001B[39mwith_traceback(\n\u001B[1;32m   1025\u001B[0m         e\u001B[38;5;241m.\u001B[39m__traceback__\n\u001B[1;32m   1026\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1028\u001B[0m signpost_event(\n\u001B[1;32m   1029\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdynamo\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   1030\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOutputGraph.call_user_compiler\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1036\u001B[0m     },\n\u001B[1;32m   1037\u001B[0m )\n\u001B[1;32m   1039\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m compiled_fn\n",
      "File \u001B[0;32m~/mambaforge/envs/pytorch-compile-condavenv-3-10/lib/python3.10/site-packages/torch/_dynamo/output_graph.py:1009\u001B[0m, in \u001B[0;36mOutputGraph.call_user_compiler\u001B[0;34m(self, gm)\u001B[0m\n\u001B[1;32m   1007\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m config\u001B[38;5;241m.\u001B[39mverify_correctness:\n\u001B[1;32m   1008\u001B[0m     compiler_fn \u001B[38;5;241m=\u001B[39m WrapperBackend(compiler_fn)\n\u001B[0;32m-> 1009\u001B[0m compiled_fn \u001B[38;5;241m=\u001B[39m \u001B[43mcompiler_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgm\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexample_inputs\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1010\u001B[0m _step_logger()(logging\u001B[38;5;241m.\u001B[39mINFO, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdone compiler function \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1011\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mcallable\u001B[39m(compiled_fn), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcompiler_fn did not return callable\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "File \u001B[0;32m~/mambaforge/envs/pytorch-compile-condavenv-3-10/lib/python3.10/site-packages/torch/_dynamo/repro/after_dynamo.py:117\u001B[0m, in \u001B[0;36mwrap_backend_debug.<locals>.debug_wrapper\u001B[0;34m(gm, example_inputs, **kwargs)\u001B[0m\n\u001B[1;32m    115\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m\n\u001B[1;32m    116\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 117\u001B[0m     compiled_gm \u001B[38;5;241m=\u001B[39m \u001B[43mcompiler_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgm\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexample_inputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    119\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m compiled_gm\n",
      "File \u001B[0;32m~/mambaforge/envs/pytorch-compile-condavenv-3-10/lib/python3.10/site-packages/torch/__init__.py:1607\u001B[0m, in \u001B[0;36m_TorchCompileWrapper.__call__\u001B[0;34m(self, model_, inputs_)\u001B[0m\n\u001B[1;32m   1606\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, model_, inputs_):\n\u001B[0;32m-> 1607\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompiler_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[26], line 4\u001B[0m, in \u001B[0;36mcustom_backend\u001B[0;34m(gm, example_inputs)\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcustom_backend\u001B[39m(gm: torch\u001B[38;5;241m.\u001B[39mfx\u001B[38;5;241m.\u001B[39mGraphModule, example_inputs: List[torch\u001B[38;5;241m.\u001B[39mTensor]):\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcustom backend called with FX graph:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m----> 4\u001B[0m     \u001B[43mgm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgraph\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprint_tabular\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m gm\u001B[38;5;241m.\u001B[39mforward\n",
      "File \u001B[0;32m~/mambaforge/envs/pytorch-compile-condavenv-3-10/lib/python3.10/site-packages/torch/fx/graph.py:1300\u001B[0m, in \u001B[0;36mGraph.print_tabular\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1294\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1295\u001B[0m \u001B[38;5;124;03mPrints the intermediate representation of the graph in tabular\u001B[39;00m\n\u001B[1;32m   1296\u001B[0m \u001B[38;5;124;03mformat. Note that this API requires the ``tabulate`` module to be\u001B[39;00m\n\u001B[1;32m   1297\u001B[0m \u001B[38;5;124;03minstalled.\u001B[39;00m\n\u001B[1;32m   1298\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1299\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1300\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtabulate\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m tabulate\n\u001B[1;32m   1301\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m:\n\u001B[1;32m   1302\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`print_tabular` relies on the library `tabulate`, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1303\u001B[0m           \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwhich could not be found on this machine. Run `pip \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1304\u001B[0m           \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minstall tabulate` to install the library.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mBackendCompilerFailed\u001B[0m: backend='custom_backend' raised:\nModuleNotFoundError: No module named 'tabulate'\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"
     ]
    }
   ],
   "source": [
    "def bar(a, b):\n",
    "    x = a / (torch.abs(a) + 1)\n",
    "    if b.sum() < 0:\n",
    "        b = b * -1\n",
    "    return x * b\n",
    "\n",
    "opt_bar = torch.compile(bar, backend=custom_backend)\n",
    "inp1 = torch.randn(10)\n",
    "inp2 = torch.randn(10)\n",
    "opt_bar(inp1, inp2)\n",
    "opt_bar(inp1, -inp2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T22:54:59.120578Z",
     "start_time": "2023-11-06T22:54:58.384613Z"
    }
   },
   "id": "8c807cd9af2b14a2"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph Count: 2\n",
      "Graph Break Count: 1\n",
      "Op Count: 6\n",
      "Break Reasons:\n",
      "  Break Reason 1:\n",
      "    Reason: generic_jump TensorVariable()\n",
      "    User Stack:\n",
      "      <FrameSummary file /var/folders/cp/df32wpxn1ps8dmn6w_tydwdr0000gn/T/ipykernel_26898/3803548536.py, line 3 in bar>\n",
      "Ops per Graph:\n",
      "  Ops 1:\n",
      "    <built-in method abs of type object at 0x113085780>\n",
      "    <built-in function add>\n",
      "    <built-in function truediv>\n",
      "    <built-in function lt>\n",
      "  Ops 2:\n",
      "    <built-in function mul>\n",
      "    <built-in function mul>\n",
      "Out Guards:\n",
      "  Guard 1:\n",
      "    Name: \"L['b']\"\n",
      "    Source: local\n",
      "    Create Function: TENSOR_MATCH\n",
      "    Guard Types: ['TENSOR_MATCH']\n",
      "    Code List: [\"hasattr(L['b'], '_dynamo_dynamic_indices') == False\"]\n",
      "    Object Weakref: <weakref at 0x28d07b830; dead>\n",
      "    Guarded Class Weakref: <weakref at 0x11748fd80; to 'torch._C._TensorMeta' at 0x11670dee0 (Tensor)>\n",
      "  Guard 2:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: GRAD_MODE\n",
      "    Guard Types: ['GRAD_MODE']\n",
      "    Code List: ['___is_grad_enabled()']\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 3:\n",
      "    Name: \"G['torch']\"\n",
      "    Source: global\n",
      "    Create Function: FUNCTION_MATCH\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 4:\n",
      "    Name: \"L['a']\"\n",
      "    Source: local\n",
      "    Create Function: TENSOR_MATCH\n",
      "    Guard Types: ['TENSOR_MATCH']\n",
      "    Code List: [\"hasattr(L['a'], '_dynamo_dynamic_indices') == False\"]\n",
      "    Object Weakref: <weakref at 0x16a7e4220; dead>\n",
      "    Guarded Class Weakref: <weakref at 0x11748fd80; to 'torch._C._TensorMeta' at 0x11670dee0 (Tensor)>\n",
      "  Guard 5:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: DETERMINISTIC_ALGORITHMS\n",
      "    Guard Types: ['DETERMINISTIC_ALGORITHMS']\n",
      "    Code List: ['not ___are_deterministic_algorithms_enabled()']\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 6:\n",
      "    Name: ''\n",
      "    Source: shape_env\n",
      "    Create Function: SHAPE_ENV\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 7:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: TORCH_FUNCTION_STATE\n",
      "    Guard Types: ['TORCH_FUNCTION_STATE']\n",
      "    Code List: ['___is_torch_function_enabled()']\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 8:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: DEFAULT_DEVICE\n",
      "    Guard Types: ['DEFAULT_DEVICE']\n",
      "    Code List: ['utils_device.CURRENT_DEVICE == None']\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 9:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: GRAD_MODE\n",
      "    Guard Types: ['GRAD_MODE']\n",
      "    Code List: ['___is_grad_enabled()']\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 10:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: DETERMINISTIC_ALGORITHMS\n",
      "    Guard Types: ['DETERMINISTIC_ALGORITHMS']\n",
      "    Code List: ['not ___are_deterministic_algorithms_enabled()']\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 11:\n",
      "    Name: \"L['b']\"\n",
      "    Source: local\n",
      "    Create Function: TENSOR_MATCH\n",
      "    Guard Types: ['TENSOR_MATCH']\n",
      "    Code List: [\"hasattr(L['b'], '_dynamo_dynamic_indices') == False\"]\n",
      "    Object Weakref: <weakref at 0x28d07b830; dead>\n",
      "    Guarded Class Weakref: <weakref at 0x11748fd80; to 'torch._C._TensorMeta' at 0x11670dee0 (Tensor)>\n",
      "  Guard 12:\n",
      "    Name: ''\n",
      "    Source: shape_env\n",
      "    Create Function: SHAPE_ENV\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 13:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: TORCH_FUNCTION_STATE\n",
      "    Guard Types: ['TORCH_FUNCTION_STATE']\n",
      "    Code List: ['___is_torch_function_enabled()']\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 14:\n",
      "    Name: \"L['x']\"\n",
      "    Source: local\n",
      "    Create Function: TENSOR_MATCH\n",
      "    Guard Types: ['TENSOR_MATCH']\n",
      "    Code List: [\"hasattr(L['x'], '_dynamo_dynamic_indices') == False\"]\n",
      "    Object Weakref: <weakref at 0x28d8a0630; dead>\n",
      "    Guarded Class Weakref: <weakref at 0x11748fd80; to 'torch._C._TensorMeta' at 0x11670dee0 (Tensor)>\n",
      "  Guard 15:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: DEFAULT_DEVICE\n",
      "    Guard Types: ['DEFAULT_DEVICE']\n",
      "    Code List: ['utils_device.CURRENT_DEVICE == None']\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "Compile Times: TorchDynamo compilation metrics:\n",
      "Function, Runtimes (s)\n",
      "_compile.<locals>.compile_inner, 0.0101, 0.0049\n",
      "OutputGraph.call_user_compiler, 0.0010, 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Reset since we are using a different backend.\n",
    "torch._dynamo.reset()\n",
    "explain_output = torch._dynamo.explain(bar)(torch.randn(10), torch.randn(10))\n",
    "print(explain_output)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T22:59:24.725672Z",
     "start_time": "2023-11-06T22:59:24.701671Z"
    }
   },
   "id": "a807f68140e90b13"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cp/df32wpxn1ps8dmn6w_tydwdr0000gn/T/ipykernel_26898/1683878851.py\", line 3, in <module>\n",
      "    opt_bar(torch.randn(10), torch.randn(10))\n",
      "  File \"/Users/brian/mambaforge/envs/pytorch-compile-condavenv-3-10/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 328, in _fn\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/Users/brian/mambaforge/envs/pytorch-compile-condavenv-3-10/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 490, in catch_errors\n",
      "    return callback(frame, cache_entry, hooks, frame_state)\n",
      "  File \"/Users/brian/mambaforge/envs/pytorch-compile-condavenv-3-10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 133, in _fn\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/Users/brian/mambaforge/envs/pytorch-compile-condavenv-3-10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 389, in _convert_frame_assert\n",
      "    return _compile(\n",
      "  File \"/Users/brian/mambaforge/envs/pytorch-compile-condavenv-3-10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 569, in _compile\n",
      "    guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
      "  File \"/Users/brian/mambaforge/envs/pytorch-compile-condavenv-3-10/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 189, in time_wrapper\n",
      "    r = func(*args, **kwargs)\n",
      "  File \"/Users/brian/mambaforge/envs/pytorch-compile-condavenv-3-10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 491, in compile_inner\n",
      "    out_code = transform_code_object(code, transform)\n",
      "  File \"/Users/brian/mambaforge/envs/pytorch-compile-condavenv-3-10/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py\", line 1028, in transform_code_object\n",
      "    transformations(instructions, code_options)\n",
      "  File \"/Users/brian/mambaforge/envs/pytorch-compile-condavenv-3-10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 458, in transform\n",
      "    tracer.run()\n",
      "  File \"/Users/brian/mambaforge/envs/pytorch-compile-condavenv-3-10/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 2074, in run\n",
      "    super().run()\n",
      "  File \"/Users/brian/mambaforge/envs/pytorch-compile-condavenv-3-10/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 724, in run\n",
      "    and self.step()\n",
      "  File \"/Users/brian/mambaforge/envs/pytorch-compile-condavenv-3-10/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 688, in step\n",
      "    getattr(self, inst.opname)(inst)\n",
      "  File \"/Users/brian/mambaforge/envs/pytorch-compile-condavenv-3-10/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 370, in inner\n",
      "    raise exc.UserError(\n",
      "torch._dynamo.exc.UserError: Dynamic control flow is not supported at the moment. Please use functorch.experimental.control_flow.cond to explicitly capture the control flow\n",
      "\n",
      "from user code:\n",
      "   File \"/var/folders/cp/df32wpxn1ps8dmn6w_tydwdr0000gn/T/ipykernel_26898/3803548536.py\", line 3, in bar\n",
      "    if b.sum() < 0:\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "opt_bar = torch.compile(bar, fullgraph=True)\n",
    "try:\n",
    "    opt_bar(torch.randn(10), torch.randn(10))\n",
    "except:\n",
    "    tb.print_exc()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T22:59:45.713671Z",
     "start_time": "2023-11-06T22:59:45.694129Z"
    }
   },
   "id": "895d5f58e050c459"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[30], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m opt_model \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcompile(\u001B[43minit_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m, fullgraph\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(opt_model(generate_data(\u001B[38;5;241m16\u001B[39m)[\u001B[38;5;241m0\u001B[39m]))\n",
      "Cell \u001B[0;32mIn[10], line 25\u001B[0m, in \u001B[0;36minit_model\u001B[0;34m()\u001B[0m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minit_model\u001B[39m():\n\u001B[0;32m---> 25\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mdensenet121\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat32\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcuda\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/mambaforge/envs/pytorch-compile-condavenv-3-10/lib/python3.10/site-packages/torch/nn/modules/module.py:918\u001B[0m, in \u001B[0;36mModule.cuda\u001B[0;34m(self, device)\u001B[0m\n\u001B[1;32m    901\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcuda\u001B[39m(\u001B[38;5;28mself\u001B[39m: T, device: Optional[Union[\u001B[38;5;28mint\u001B[39m, device]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m T:\n\u001B[1;32m    902\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Moves all model parameters and buffers to the GPU.\u001B[39;00m\n\u001B[1;32m    903\u001B[0m \n\u001B[1;32m    904\u001B[0m \u001B[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    916\u001B[0m \u001B[38;5;124;03m        Module: self\u001B[39;00m\n\u001B[1;32m    917\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 918\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcuda\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/mambaforge/envs/pytorch-compile-condavenv-3-10/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn, recurse)\u001B[0m\n\u001B[1;32m    808\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m recurse:\n\u001B[1;32m    809\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[0;32m--> 810\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    812\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[1;32m    813\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[1;32m    814\u001B[0m         \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[1;32m    815\u001B[0m         \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    820\u001B[0m         \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[1;32m    821\u001B[0m         \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "File \u001B[0;32m~/mambaforge/envs/pytorch-compile-condavenv-3-10/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn, recurse)\u001B[0m\n\u001B[1;32m    808\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m recurse:\n\u001B[1;32m    809\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[0;32m--> 810\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    812\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[1;32m    813\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[1;32m    814\u001B[0m         \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[1;32m    815\u001B[0m         \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    820\u001B[0m         \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[1;32m    821\u001B[0m         \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "File \u001B[0;32m~/mambaforge/envs/pytorch-compile-condavenv-3-10/lib/python3.10/site-packages/torch/nn/modules/module.py:833\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn, recurse)\u001B[0m\n\u001B[1;32m    829\u001B[0m \u001B[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001B[39;00m\n\u001B[1;32m    830\u001B[0m \u001B[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001B[39;00m\n\u001B[1;32m    831\u001B[0m \u001B[38;5;66;03m# `with torch.no_grad():`\u001B[39;00m\n\u001B[1;32m    832\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m--> 833\u001B[0m     param_applied \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparam\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    834\u001B[0m should_use_set_data \u001B[38;5;241m=\u001B[39m compute_should_use_set_data(param, param_applied)\n\u001B[1;32m    835\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m should_use_set_data:\n",
      "File \u001B[0;32m~/mambaforge/envs/pytorch-compile-condavenv-3-10/lib/python3.10/site-packages/torch/nn/modules/module.py:918\u001B[0m, in \u001B[0;36mModule.cuda.<locals>.<lambda>\u001B[0;34m(t)\u001B[0m\n\u001B[1;32m    901\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcuda\u001B[39m(\u001B[38;5;28mself\u001B[39m: T, device: Optional[Union[\u001B[38;5;28mint\u001B[39m, device]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m T:\n\u001B[1;32m    902\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Moves all model parameters and buffers to the GPU.\u001B[39;00m\n\u001B[1;32m    903\u001B[0m \n\u001B[1;32m    904\u001B[0m \u001B[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    916\u001B[0m \u001B[38;5;124;03m        Module: self\u001B[39;00m\n\u001B[1;32m    917\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 918\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_apply(\u001B[38;5;28;01mlambda\u001B[39;00m t: \u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcuda\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[0;32m~/mambaforge/envs/pytorch-compile-condavenv-3-10/lib/python3.10/site-packages/torch/cuda/__init__.py:289\u001B[0m, in \u001B[0;36m_lazy_init\u001B[0;34m()\u001B[0m\n\u001B[1;32m    284\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    285\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    286\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmultiprocessing, you must use the \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mspawn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m start method\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    287\u001B[0m     )\n\u001B[1;32m    288\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(torch\u001B[38;5;241m.\u001B[39m_C, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_cuda_getDeviceCount\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m--> 289\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTorch not compiled with CUDA enabled\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    290\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _cudart \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    291\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\n\u001B[1;32m    292\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    293\u001B[0m     )\n",
      "\u001B[0;31mAssertionError\u001B[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "opt_model = torch.compile(init_model(), fullgraph=True)\n",
    "print(opt_model(generate_data(16)[0]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T23:00:00.225576Z",
     "start_time": "2023-11-06T23:00:00.080691Z"
    }
   },
   "id": "1629683bea156850"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ef5b29909a2f3abd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pytorch-compile-condavenv-3-10",
   "language": "python",
   "display_name": "pytorch-compile-condavenv-3-10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
